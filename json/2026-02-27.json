{
  "019939515ffb1d62d4f01800097c5f0fd7977d6d": {
    "id": "019939515ffb1d62d4f01800097c5f0fd7977d6d",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "jeffgeerling.com",
    "title": "Upgrading my Open Source Pi Surveillance Server with Frigate",
    "url": "https://www.jeffgeerling.com/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/",
    "published_at": "2026-02-27T15:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "sv",
    "content": "# Upgrading my Open Source Pi Surveillance Server with Frigate\n\nFeb 27, 2026\n\nIn 2024 I built a Pi Frigate NVR with Axzez's Interceptor 1U Case, and installed it in my 19\" rack. Using a Coral TPU for object detection, it's been dutifully surveilling my property—on *my* terms (100% local, no cloud integration or account required).\n\n![Exaviz Cruiser CM5 carrier board inside DeskPi mini rack enclosure with Annke 4K camera on top](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-mini-rack-enclosure-with-annke-camera.jpeg)\n\nI've wanted to downsize the setup while keeping ~~cheap~~ large hard drives1, and an AI accelerator.\n\nLuckily, Exaviz sent me their new Cruiser board to test, and DeskPi sent me a prototype mini rack enclosure for it.\n\nI bought a couple Dell R720 drive sleds, plugged in a Compute Module 5, and tested it. I made a video on the upgrade here:\n\nIf you'd rather read through a more condensed version, scroll on!\n\n## Hardware\n\nThe star of the show, is of course the Cruiser CM5 carrier board:\n\n![Exaviz Cruiser CM5 carrier board top](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-top.jpeg)\n\nThe architecture works around the Raspberry Pi CM5's main downside: limited PCI Express bandwidth. Instead of adding an expensive PCIe switch, and attaching multiple high-bandwidth devices to the Pi's only PCIe lane, Exaviz went with a mix of PCIe and USB:\n\n* The M.2 M-key NVMe slot is connected directly to the Pi's PCIe Gen 2 x1 lane (technically you can run it at Gen 3, but that's not the official spec).\n* The 2.5 Gbps WAN port (RTL8156BG) is routed through USB 3.0\n* The 2x SATA connections (JMS561) are routed through USB 3.0\n* The (up to) 8 10/100/1000 Mbps PoE+ ports (RTL8367RB) are connected to the CM5's built-in 1 Gbps Ethernet.\n\nThere are extra USB 3.0 and USB 2.0 ports for accessories and peripherals, a microSD card slot for Lite CM5s, two Qwiic I2C connectors, two HDMI 2.0 ports, fan headers, a molex power connector for HDD power, and even an ESP32-C6 thrown in the mix to give the board Zigbee (or additional WiFi/BT) capabilities.\n\nThere's a front panel IO header, a jumper to enable hardware RAID if you want, a connector for adding on even *more* PoE ports (via addon card), and a 48V DC barrel jack accepting up to 288W of power (48V at 6A, recommended if you buy the maxed-out version).\n\nThe board doesn't fit in an ITX-sized chassis, but all the important IO is on one side, at least, meaning you don't wind up with a cable mess.\n\n## DeskPi's mini rack case\n\nInstalling the Cruiser in DeskPi's prototype mini rack enclosure (which is not public yet—I'll try to link to it when available) was easy enough:\n\n![Exaviz Cruiser in DeskPi Rackmount - front](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-front-built.jpg)\n\nThe front side looks polished, with the ports all in a wide IO cutout, and the R720-style drive sleds locked in.\n\nThe back... not quite as much:\n\n![Exaviz Cruiser in DeskPi Rackmount - back](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-back-built.jpg)\n\nDeskPi is working on it, though. This prototype just lets the cables dangle, but they may be engineering a PCB that allows the drive sleds to hot swap more easily, without having wires to plug and unplug by hand every time.\n\nThey also included a power button with no power LED, which makes it harder to tell if the system is powered on from the front—so hopefully the final version will have an LED on the power button, in addition to a better option for cable managing the two drives.\n\n(I've also mentioned thin ITX motherboard compatibility would make this enclosure even better!)\n\nThe two bays at the bottom accept drive sleds like these Dell R720-compatible sleds I found on eBay. Into those sleds I installed two $99(!) 4TB IronWolf NAS hard drives. Someday storage will go back down in price again. Hopefully.\n\nThe power button and cabling are cosmetic issues, but cooling is something that could be improved in the design that goes to production.\n\n![Exaviz Cruiser CM5 is hot in prototype case](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-deskpi-case-cm5-hot-thermal-camera.jpg)\n\nThe fan up top is used for exhaust, but as you can see above, because there are so many vents and openings, air is not directed over the top of the hottest part of the build—the CM5, and thus it can overheat under load (even with a small heatsink, as I had configured it).\n\nFor now, I can work around this problem by installing a fan/heatsink combo like the EDAtec's Active Cooler for CM5. The Cruiser includes multiple case fan (3 pin) headers, so adding a couple fans elsewhere would be another easy fix.\n\n## Software\n\nMy goal was to run Frigate, one of the most popular open source NVR apps, and to do that efficiently, you need mass storage (to store video) and a suitable accelerator for object detection (e.g. people, cars, bikes, animals, etc.).\n\nTo that end:\n\n* I built a RAID 1 array with mdadm, with the two 4TB drives mirrored, so if one drive dies, I still have all the footage.\n* I installed the Hailo 8 driver on the Pi following Frigate's guide.\n* I installed Frigate (in Docker) using my pi-nvr Ansible playbook.\n\nAfter sorting out one issue with the Hailo on the Pi's PCIe bus (mentioned in the linked post above), everything was working, and Frigate saw the three cameras I had connected:\n\n![Frigate with three camera feeds including a 4K Annke camera](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/frigate-three-cameras-feed-exaviz-cruiser.jpg)\n\nThree cameras barely scratches the surface of what this setup can do—CPU usage was under 5%, the Hailo utilization was below 10%, and object detection was running around 10-11ms. That was with two 1080p cameras and one 4K.\n\nConsidering even 4K security cameras run on 100 Mbps networking, and most cameras send low-bandwidth H.264 or H.265 feeds, 8 cameras should be easy, even if you're just running one slow hard drive and a Hailo 8L.\n\n## PoE ports and power monitoring and control\n\nThe headline feature of this board is the built-in PoE+ switch—which is managed through Linux on the Pi. Exaviz maintains their own OS image, but you can install their drivers on Pi OS, like I did.\n\nIn addition to configuring all the networking, their packages include a GUI for PoE port management, called PoE Tool:\n\n![Exaviz Cruiser PoE Tool GUI in Pi OS](/blog/2026/upgrading-my-open-source-pi-surveillance-server-frigate/exaviz-cruiser-poe-tool.jpg)\n\nYou can monitor port status, power consumption, and reset ports (to remotely power cycle powered devices).\n\nThe default networking configuration uses the Pi as a bridge (with an uplink to your network through the 2.5 Gbps WAN port). The PoE ports are on their own subnet. All of this can be customized, if you don't like the defaults configured by the `exaviz-netplan` package.\n\nIf you'd like to monitor the PoE port status in Home Assistant, Exaviz maintains a plugin for that. I'd love to see more vendors provide HA integration for devices which will find their way into homelabs.\n\n## Conclusion\n\nBesides the cooling issue, I had no issues with the pre-launch hardware I tested. I was especially impressed by the quality of Exaviz's documentation, even though they hadn't publicly launched the product while I was testing.\n\nIf DeskPi can iron out a couple enclosure quirks, this will be a killer setup for network video recording, or even a generic storage server for mini racks.\n\nThe Crusier will go on sale for between $99 and $149, and Exaviz even makes their own deep 1U desktop case (that fits in a mini rack).\n\n---\n\n1. Hard drives are still the gold standard for Network Video Recorders (NVRs). They're cheap (at least compared to SSDs—nothing's cheap these days), and plenty fast for low-bandwidth video streams. Not amazing for random access, of course. ↩︎\n\n## Further reading:\n\n* The first good Raspberry Pi Laptop\n* Raspberry Pi CM5 is 2-3x faster, drop-in upgrade (mostly)\n* Building a Pi Frigate NVR with Axzez's Interceptor 1U Case\n\n## Comments",
    "summary": "In 2024 I built a Pi Frigate NVR with Axzez's Interceptor 1U Case , and installed it in my 19\" rack. Using a Coral TPU for object detection, it's been dutifully surveilling my property—on my terms (100% local, no cloud integration or account required). I've wanted to downsize the setup while keeping cheap large hard drives 1 , and an AI accelerator.",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 6
    }
  },
  "6f086679fe9eb5e014357673cf431741c1356059": {
    "id": "6f086679fe9eb5e014357673cf431741c1356059",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "shkspr.mobi",
    "title": "Book Review: Weird Things Customers Say in Bookshops by Jen Campbell ★★☆☆☆",
    "url": "https://shkspr.mobi/blog/2026/02/book-review-weird-things-customers-say-in-bookshops-by-jen-campbell/",
    "published_at": "2026-02-27T12:34:11+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "no",
    "content": "![Book cover](https://shkspr.mobi/blog/wp-content/uploads/2026/03/1366054116.webp)\n\nRemember back in the early 2010s when any moderately popular Twitter account could become a book (or even a TV series)?\n\nThis is a collection of Tweet-sized \"overheard in\" stories. All set in book shops.\n\nIsn't it funny that some people don't know how books work! ROFL!\n\nAren't the general public strange? LOLOL!\n\nThat's a bit harsh of me. It only rarely becomes mean-spirited. But in a book this short, it rather contaminates the joy.\n\nThat said, this one will live rent-free in my head for a while:\n\n> Did Beatrix Potter ever write a book about dinosaurs?\n\nIt's the sort of stocking-filler book which is reasonable for perusing on the loo. Light-hearted but ultimately disposable.\n\nStill, at least Neil Gaiman found it funny enough to leave a blurb…\n\n| Verdict |\n| --- |\n| ★★☆☆☆ Disappointing |\n\n### Get the book\n\nSupport my blog by using these affiliate links:\n\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/ethicalbooksearch.png)Ethical Book Search\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/amazon.svg)Read on Amazon Kindle\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/kobo.webp)Audiobook and ePub from Kobo\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/alibris.svg)Used from Alibris\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/audible.webp)Listen on Audible\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/bookshop.svg)Buy from an independent bookshop\n* ![](https://shkspr.mobi/favicons/?domain=www.jen-campbell.co.uk)Author's homepage\n* ![](https://shkspr.mobi/favicons/?domain=www.jen-campbell.co.uk)Publisher's details\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/worldcat.png)Borrow from your local library\n* ![](https://shkspr.mobi/blog/wp-content/themes/edent-wordpress-theme/assets/images/openlibrary.webp)ISBN: 9781780335148\n\n---\n\n## Share this post on…\n\n* ![Mastodon](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Mastodon%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%20fill%3D%22%23fff%22%3E%3Crect%20width%3D%22512%22%20height%3D%22512%22%20fill%3D%22url%28%23a%29%22%2F%3E%3ClinearGradient%20id%3D%22a%22%20y2%3D%221%22%3E%3Cstop%20offset%3D%220%22%20stop-color%3D%22%236364ff%22%2F%3E%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23563acc%22%2F%3E%3C%2FlinearGradient%3E%3Cpath%20d%3D%22M317%20381q-124%2028-123-39%2069%2015%20149%202%2067-13%2072-80%203-101-3-116-19-49-72-58-98-10-162%200-56%2010-75%2058-12%2031-3%20147%203%2032%209%2053%2013%2046%2070%2069%2083%2023%20138-9%22%2F%3E%3Cpath%20d%3D%22M360%20293h-36v-93q-1-26-29-23-20%203-20%2034v47h-36v-47q0-31-20-34-30-3-30%2028v88h-36v-91q1-51%2044-60%2033-5%2051%2021l9%2015%209-15q16-26%2051-21%2043%209%2043%2060%22%20fill%3D%22url%28%23a%29%22%2F%3E%3C%2Fsvg%3E)\n* ![Facebook](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Facebook%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20fill%3D%22%231877f2%22%20d%3D%22M0%200h512v512H0z%22%2F%3E%3Cpath%20fill%3D%22%23fff%22%20d%3D%22m356%20330%2011-74h-71v-48c0-20%2010-40%2042-40h32v-63s-29-5-57-5c-59%200-97%2035-97%20100v56h-65v74h65v182h80V330h60z%22%2F%3E%3C%2Fsvg%3E)\n* ![LinkedIn](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22LinkedIn%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%20fill%3D%22%23fff%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%230077b5%22%2F%3E%3Ccircle%20cx%3D%22142%22%20cy%3D%22138%22%20r%3D%2237%22%2F%3E%3Cpath%20stroke%3D%22%23fff%22%20stroke-width%3D%2266%22%20d%3D%22M244%20194v198M142%20194v198%22%2F%3E%3Cpath%20d%3D%22M276%20282c0-20%2013-40%2036-40%2024%200%2033%2018%2033%2045v105h66V279c0-61-32-89-76-89-34%200-51%2019-59%2032%22%2F%3E%3C%2Fsvg%3E)\n* ![BlueSky](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Bluesky%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%231185fe%22%2F%3E%3Cpath%20d%3D%22M159%20126c39%2029%2082%2089%2097%20121%2015-32%2058-92%2097-121%2028-22%2074-38%2074%2014%200%2011-6%2088-9%20101-13%2043-57%2054-97%2048%2069%2011%2087%2050%2049%2089-72%2075-104-18-112-42l-2-5-2%205c-8%2024-40%20117-112%2042-38-39-20-78%2049-89-40%206-84-5-97-48-3-13-9-90-9-101%200-52%2046-36%2074-14z%22%20fill%3D%22%23fff%22%2F%3E%3C%2Fsvg%3E)\n* ![Threads](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Threads%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%2F%3E%3Cpath%20stroke%3D%22%23fff%22%20stroke-width%3D%2234.5%22%20d%3D%22m200.7%20200c12.3-18%2033.3-32%2066.3-29s63%2022%2061.2%2086-29.2%2086-67.3%2087.4-61-21.5-61.6-45.8%2015.2-53.7%2079.2-52.6%20110.7%2030.5%20113.7%2084-46%20108.5-133.2%20108-156-50-156.5-179S173%2079%20256%2079s134%2041%20153.2%20111.3%22%2F%3E%3C%2Fsvg%3E)\n* ![Reddit](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Reddit%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%23f40%22%2F%3E%3Cg%20fill%3D%22%23fff%22%3E%3Cellipse%20cx%3D%22256%22%20cy%3D%22307%22%20rx%3D%22166%22%20ry%3D%22117%22%2F%3E%3Ccircle%20cx%3D%22106%22%20cy%3D%22256%22%20r%3D%2242%22%2F%3E%3Ccircle%20cx%3D%22407%22%20cy%3D%22256%22%20r%3D%2242%22%2F%3E%3Ccircle%20cx%3D%22375%22%20cy%3D%22114%22%20r%3D%2232%22%2F%3E%3C%2Fg%3E%3Cg%20stroke-linecap%3D%22round%22%20stroke-linejoin%3D%22round%22%20fill%3D%22none%22%3E%3Cpath%20d%3D%22m256%20196%2023-101%2073%2015%22%20stroke%3D%22%23fff%22%20stroke-width%3D%2216%22%2F%3E%3Cpath%20d%3D%22m191%20359c33%2025%2097%2026%20130%200%22%20stroke%3D%22%23f40%22%20stroke-width%3D%2213%22%2F%3E%3C%2Fg%3E%3Cg%20fill%3D%22%23f40%22%3E%3Ccircle%20cx%3D%22191%22%20cy%3D%22287%22%20r%3D%2231%22%2F%3E%3Ccircle%20cx%3D%22321%22%20cy%3D%22287%22%20r%3D%2231%22%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E)\n* ![HackerNews](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Hacker%20News%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%23f60%22%2F%3E%3Cpath%20fill%3D%22%23fff%22%20d%3D%22m124%2091h51l81%20162%2081-164h51L276%20293v136h-40V293%22%2F%3E%3C%2Fsvg%3E)\n* ![Lobsters](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Lobste.rs%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%23ac130c%22%2F%3E%3Cpath%20d%3D%22M421.1%20312.9H398.9c0%2012-5.8%2035.2-10.6%2045-8.8%2017.7-26.8%2032.2-48.4%2036.6-20.8%203.8-41.6%203.2-64.9%203.4-32.4-5.8-42.1-23.7-41.2-57.6V157c-.1-18.6-2.4-45.3%2021-51.5%206.3-1.5%2019.6-2.4%2029.8-2.7v-21H114v21c8.9.6%2019.5%201.6%2024.9%203.1%2022%204.8%2022.4%2026.7%2023.9%2047.9V353.6c0%2010.4-1.1%2018.9-2.3%2025.5-2.4%2012-10.9%2019.5-23.9%2022.8-6.3%201.5-14.4%202.4-24.2%202.7v23.5H421Z%22%20fill%3D%22%23fff%22%2F%3E%3C%2Fsvg%3E)\n* ![WhatsApp](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22WhatsApp%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Cpath%20d%3D%22m0%200H512V512H0%22%20fill%3D%22%2325d366%22%2F%3E%3Cpath%20fill%3D%22%23fff%22%20d%3D%22m79%20434%2025.7-93.9a181.1%20181.2%200%201170.3%2068.7M122.5%20391l57-15a150.6%20150.6%200%2010-41.8-40.6m93-127c2%205%200%2010-11%2022.2-6%206-4%208%206.6%2023s28%2029%2044%2036.5%2015%207%2021.7-1c15-17%2011-21%2026-14.2l27%2013c8%204%208.4%204%208.5%209s-1.7%2018-7%2023.6-25%2024.8-60%2012-59-23-99-77-1.6-86%203.6-88%207-1.5%2017-1.3q4%200%207%205%22%2F%3E%3C%2Fsvg%3E)\n* ![Telegram](data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20aria-label%3D%22Telegram%22%20role%3D%22img%22%20viewBox%3D%220%200%20512%20512%22%3E%3Crect%20width%3D%22512%22%20height%3D%22512%22%20fill%3D%22%2337aee2%22%2F%3E%3Cpath%20fill%3D%22%23c8daea%22%20d%3D%22M199%20404c-11%200-10-4-13-14l-32-105%20245-144%22%2F%3E%3Cpath%20fill%3D%22%23a9c9dd%22%20d%3D%22M199%20404c7%200%2011-4%2016-8l45-43-56-34%22%2F%3E%3Cpath%20fill%3D%22%23f6fbfe%22%20d%3D%22M204%20319l135%2099c14%209%2026%204%2030-14l55-258c5-22-9-32-24-25L79%20245c-21%208-21%2021-4%2026l83%2026%20190-121c9-5%2017-3%2011%204%22%2F%3E%3C%2Fsvg%3E)",
    "summary": "Remember back in the early 2010s when any moderately popular Twitter account could become a book (or even a TV series)?  This is a collection of Tweet-sized \"overheard in\" stories. All set in book shops.  Isn't it funny that some people don't know how books work! ROFL!  Aren't the general public strange? LOLOL!  That's a bit harsh of me. It only rarely becomes mean-spirited. But in a book this…",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "059c57d6e14551367e4a5d8706f7a2f10b245f13": {
    "id": "059c57d6e14551367e4a5d8706f7a2f10b245f13",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "devblogs.microsoft.com/oldnewthing",
    "title": "Intercepting messages inside Is­Dialog­Message, fine-tuning the message filter",
    "url": "https://devblogs.microsoft.com/oldnewthing/20260227-00/?p=112094",
    "published_at": "2026-02-27T15:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "Last time, we  used a `MSGF_DIALOG­BOX` message filter to hook into the `Is­Dialog­Message` so that we had the option to grab the `ESC` before it gets turned into an `IDCANCEL`. There are some problems with our initial foray.\n\nOne is the problem of recursive dialogs. If the first dialog shows another copy of itself (for example, a certificate dialog showing a dialog for its parent certificate), then the thread-local variable gets overwritten, and the first dialog’s information is lost.\n\nWe could solve that by having each dialog remember the original value and restore it when the dialog dismisses. Alternatively, we could maintain an explicit stack of dialogs, pushing when a new dialog is created and popping when it is destroyed.\n\nHowever, this fails to handle the case where the dialog is modeless. In that case, the two dialogs could be running concurrently rather than recursively. Instead of a stack, we really need a per-thread *set* of active dialogs.\n\nAnother thing to worry about is that if this code is put into a static library, and two components in the same thread both use that static library, then you have to be careful that the two copies of the library don’t conflict with each other.\n\nI came up with this initial idea:\n\n```\n#define DIALOG_WANTS_ESC_PROP TEXT(\"DialogWantsEsc\")\r\n\r\nLRESULT CALLBACK DialogEscHookProc(int nCode, WPARAM wParam, LPARAM lParam)\r\n{\r\n    if (nCode == MSGF_DIALOGBOX) {\r\n        auto msg = (MSG*)lParam;\r\n        if (msg->message == WM_KEYDOWN &&\r\n            msg->wParam == VK_ESCAPE) {\r\n            auto hdlg = GetAncestor(msg->hwnd, GA_ROOT);\r\n            auto customMessage = PtrToUint(GetProp(hdlg,\r\n                                           DIALOG_WANTS_ESC_PROP));\r\n            if (customMessage &&\r\n                !(SendMessage(msg->hwnd, WM_GETDLGCODE,\r\n                             msg->wParam, lParam) &\r\n                         (DLGC_WANTALLKEYS | DLGC_WANTMESSAGE))) {\r\n                return SendMessage(hdlg, customMessage, 0, lParam);\r\n            }\r\n        }\r\n    }\r\n    return CallNextHookEx(nullptr, nCode, wParam, lParam);\r\n}\r\n```\n\nThe idea here is that instead of having to manage a table of per-thread registrations, we just let dialogs self-register by setting the `DIALOG_WANTS_ESC_PROP` property to the message number they want to receive when the user presses `ESC`.\n\nIf there are two copies of this hook installed, then the `Dialog­Esc­Hook­Proc` is called twice. The first one sends the custom message and gets the dialog’s response, and returns it; it never passes the message down the hook chain. Therefore, the second and subsequent hooks never get to run, so we don’t have a problem of the custom message getting sent multiple times for the same call to `Is­Dialog­Message`.\n\nThis design has the advantage that multiple DLLs using this pattern can coexist because the first hook (whichever it is) does all the work for everybody.\n\nAn alternate, more complex, design would pass the call down the chain if the dialog box declined to handle the `ESC` key, in case some other hook wanted to do something special. The catch is that if there are multiple copies of this hook installed, each one will send the custom message to the dialog, which would be bad if the handler for the custom message had side effects like showing a confirmation dialog.\n\nSo we can add the rule that the custom message must be safe to call multiple times if it returns `FALSE`. This means that if it wants to display a confirmation dialog, it should always return `TRUE` even if the user cancels.\n\n```\nLRESULT CALLBACK DialogEscHookProc(int nCode, WPARAM wParam, LPARAM lParam)\r\n{\r\n    if (code == MSGF_DIALOGBOX) {\r\n        auto msg = (MSG*)lParam;\r\n        if (msg->message == WM_KEYDOWN &&\r\n            msg->wParam == VK_ESCAPE) {\r\n            auto hdlg = GetAncestor(msg->hwnd, GA_ROOT);\r\n            auto customMessage = PtrToUInt(GetProp(hdlg,\r\n                                           DIALOG_WANTS_ESC_PROP));\r\n            if (customMessage &&\r\n                !(SendMessage(msg->hwnd, WM_GETDLGCODE,\r\n                             msg->wParam, msg) &\r\n                         (DLGC_WANTALLKEYS | DLGC_WANTMESSAGE)) &&\r\n                 SendMessage(hdlg, customMessage, 0, lParam)) {\r\n                 return TRUE;                                  \r\n            }\r\n        }\r\n    }\r\n    return CallNextHookEx(nullptr, nCode, wParam, lParam);\r\n}\r\n```\n\nOr we can have the first hook leave a note for the other hooks that the message has already been handled and that they shouldn’t try to handle it again.\n\n```\n#define DIALOG_WANTS_ESC_PROP TEXT(\"DialogWantsEsc\")\r\n#define CURRENT_MESSAGE_PROP TEXT(\"DialogWantsEscCurrentMessage\")\r\n\r\nLRESULT CALLBACK DialogEscHookProc(int nCode, WPARAM wParam, LPARAM lParam)\r\n{\r\n    if (code == MSGF_DIALOGBOX) {\r\n        auto msg = (MSG*)lParam;\r\n        if (msg->message == WM_KEYDOWN &&\r\n            msg->wParam == VK_ESCAPE) {\r\n            auto hdlg = GetAncestor(msg->hwnd, GA_ROOT);\r\n            auto customMessage = PtrToUInt(GetProp(hdlg,\r\n                                           DIALOG_WANTS_ESC_PROP));\r\n            if (customMessage) {\r\n                auto previous = GetProp(hdlg, CURRENT_MESSAGE_PROP);\r\n                if (previous != msg &&                              \r\n                    !(SendMessage(msg->hwnd, WM_GETDLGCODE,\r\n                                 msg->wParam, msg) &\r\n                             (DLGC_WANTALLKEYS | DLGC_WANTMESSAGE))) {\r\n                    return SendMessage(hdlg, customMessage, 0, lParam);\r\n                }\r\n                SetProp(hdlg, CURRENT_MESSAGE_PROP, msg);                     \r\n                auto result = CallNextHookEx(nullptr, nCode, wParam, lParam); \r\n                SetProp(hdlg, CURRENT_MESSAGE_PROP, previous);                \r\n                return result;                                                \r\n            }\r\n        }\r\n    }\r\n    return CallNextHookEx(nullptr, nCode, wParam, lParam);\r\n}\r\n```\n\nThe first hook will send the message to the dialog. and if the dialog declines to handle it, it passes the messages to the other hooks, but setes the “current message” property to the message that was already handled, so that other hooks won’t try to handle it again.\n\nThe last part of the puzzle is installing the hook. Since we are assuming that we cannot alter the dialog loop, the hook has to be installed by the dialog itself.\n\nLet’s assume that this dialog box already allocates other dialog state, so we can add the hook handle to the state structure.\n\n```\nstruct DIALOGSTATE\r\n{\r\n    wil::unique_hhook escapeHook;\r\n    ⟦ other stuff ⟧\r\n};\r\n\r\n// each dialog can choose its own custom message\r\n#define DM_ESCPRESSED (WM_USER+1000)\r\n\r\nINT_PTR CALLBACK DialogProc(HWND hdlg, UINT message, WPARAM wParam, LPARAM lParam)\r\n{\r\n    switch (message) {\r\n    case WM_INITDIALOG:\r\n        {\r\n            DIALOGSTATE* state = new(std:nothrow) DIALOGSTATE();\r\n            if (!state) { EndDialog(hdlg, -1); return FALSE; }\r\n            SetWindowLongPtr(hdlg, DWLP_USER, (LONG_PTR)state);\r\n            state->escapeHook.reset(SetWindowsHookEx(WM_MSGFILTER,     \r\n                             DialogEscHookProc,                        \r\n                             nullptr, GetCurrentThreadId()));          \r\n            SetProp(hdlg, DIALOG_WANTS_ESC_PROP,                       \r\n                    IntToPtr(DM_ESCPRESSED));                          \r\n            ⟦ other dialog initialization as before ⟧\r\n            ⟦ ending with \"return (whatever)\" ⟧\r\n        }\r\n\r\n    case DM_ESCPRESSED:\r\n        if (⟦ we want to process the ESC key ourselves ⟧) {\r\n            ⟦ do custom ESC key processing ⟧\r\n            SetWindowLongPtr(hdlg, DWLP_MSGRESULT, TRUE);\r\n            return TRUE;\r\n        }\r\n        break;\r\n\r\n    case WM_DESTROY:\r\n        {\r\n            auto state = (DLGSTATE*)GetWindowLongPtr(hdlg, DWLP_USER);\r\n            delete state;\r\n        }\r\n        break;\r\n\r\n    ⟦ handle other messages ⟧\r\n    }\r\n    return FALSE;\r\n}\r\n```\n\nThe dialog installs the hook when it is created and removes it when it is destroyed. The hook has become an implementation detail of the dialog.\n\nNow, I don’t recommend doing all this. Better is to just treat with the `ESC` like any other press of the (possibly imaginary) Cancel button. One of the few scenarios I can think of where this could be useful is if you want to display an extra confimation for the Close button (since its meaning is potentially ambiguous). This is still nonstandard, but at least it’s not *too* nonstandard. And for that, you can just intercept `WM_CLOSE` instead of trying to intercept the `ESC`. Intercepting the `ESC` was really just an excuse to show off message filters, which tend to be unappreciated.",
    "summary": "Making sure it triggers when you need it, and not when you don't. The post Intercepting messages inside <CODE>Is­Dialog­Message</CODE>, fine-tuning the message filter appeared first on The Old New Thing .",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 5
    }
  },
  "06c630f97149e4a35b86e18d08a2161c3b033a5f": {
    "id": "06c630f97149e4a35b86e18d08a2161c3b033a5f",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "idiallo.com",
    "title": "We Need Process, But Process Gets in the Way",
    "url": "https://idiallo.com/blog/when-process-get-in-the-way?src=feed",
    "published_at": "2026-02-27T12:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "How do you manage a company with 50,000 employees? You need processes that give you visibility and control across every function such as technology, logistics, operations, and more. But the moment you try to create a single process to govern everyone, it stops working for anyone.\n\nOne system can't cater to every team, every workflow, every context. When implemented you start seeing in-fighting, projects missing deadlines, people quitting. Compromises get made, and in my experience, it almost always becomes overwhelming.\n\nThe first time I was part of a merger, I was naïve about how it would go. The narrative we were sold was reassuring. The larger company was acquiring us because we were successful. The last thing they'd want to do was get in the way of that success. But that's not how it went.\n\nIt doesn't matter what made you successful before you join a larger organization. The principles and processes of the acquiring company are what will dominate. Your past success is acknowledged, maybe even celebrated, but it doesn't protect you from assimilation.\n\nOne of the first things we had to adopt was Scrum. It may be standard practice now, but at the time it was still making its way through the industry. Our team, developers and product managers, already had a process that worked. We knew how to communicate, how to prioritize, how to ship. Adopting this new set of ceremonies felt counterproductive. It didn't make us faster. It didn't improve communication. What it did do was increase administrative overhead. Standups, sprints, retrospectives, layer after layer of structure added on top of work that was already getting done.\n\nBut there was no going back. We were never going to return to being that nimble, ad hoc team that could resolve issues quickly and move on. We had to adopt methods that got in the way.\n\nEventually, we adapted. We adopted the process. And in doing so, we became less efficient at the local level. A lot of people, frustrated by the slowdown, left for other opportunities.\n\nBut as far as the larger company was concerned, that was acceptable. Our product was just one of many in their portfolio. Slowing down one team to get everyone aligned was a price they were willing to pay. It wasn't efficient, but it was manageable from their perspective. The math made sense at the organizational level, even if it felt like a loss from where we were standing.\n\nI understand that logic. I just don't think it's the best way forward.\n\n![CPU](https://cdn.idiallo.com/images/assets/624/cpu.jpg)\n\nThink about how a computer works. A CPU doesn't concern itself with how a hard drive retrieves data. Whether it's spinning magnetic disks or a solid state drive, the internal mechanics are irrelevant to the CPU. All it knows is that it can make a request, and the response will come back in the expected format. If the CPU had to get involved in the actual process of fetching data, it would waste enormous processing power on something that isn't its concern.\n\nOrganizations can work the same way.\n\nRather than imposing a single process across every team, a company can treat its departments as independent components. You make a request, the department delivers an output. How they produce that output like what tools they use, how they run their meetings, how they structure their work, that shouldn't be a concern, as long as the result meets the requirement.\n\nThere are places where unified processes make sense. Legal and compliance, for example, probably need to be consistent across the whole organization. But for how individual teams operate day to day, autonomy is often the better choice. Will every team's process be perfectly aligned with every other team's? No. But they'll actually work. And the people doing the work will be far less likely to walk out the door.\n\nSometimes in large organizations, it's important to identify which process works, and which team is better left alone.\n\n---\n\nDid you like this article? You can buy me a coffee.  \nShare your insightful comments here.\n\n### Join my newsletter\n\nSubscribe\n\nJavaScript is required to combat spammers.\n\nFollow me on\nTwitter,\nSpotify, or\nRSS\nFeed\n\nPrevious: When access to knowledge is no longer the limitation\n\n## On a related note, here are some interesting articles.\n\n![Vibe Coding](https://cdn.idiallo.com/images/assets/489/thumb.jpg)\n\n### Vibe Coding\n\nDescribe what you want and AI generates the entire codebase. That’s vibe coding.\nImagine you have an LLM that can build a car for you. All you have to say is “Build a car for me, here are the specs.” And it gets started right away. Before you know it, a two ton machine drops in front of you and bounces as the suspension recoils. You open the door, press on the ignition button and nothing happens. Unless you understand how cars work internally, which you don't, your option will be limited to “Rebuild the car, but with a working power button”.\n\n![Small teams should avoid Large Companies Processes](https://cdn.idiallo.com/images/assets/thumb/default-thumb-2.jpg)\n\n### Small teams should avoid Large Companies Processes\n\nWhen you're a small company, the temptation to mimic the processes of large, successful corporations can be strong. The illusion is that these workflows are inherently superior, a secret ingredient to their success. However, the reality is far more nuanced: large companies often adopt complex workflows not because they're the \"best,\" but because they have no other choice.\n\n![The real cost of Compute](https://cdn.idiallo.com/images/assets/thumb/default-thumb-2.jpg)\n\n### The real cost of Compute\n\nSomewhere along the way, we stopped talking about servers. The word felt clunky, industrial, too tied to physical reality. Instead, we started saying \"the cloud\". It sounds weightless, infinite, almost magical. Your photos live in the cloud. Your documents sync through the cloud. Your company's entire infrastructure runs in the cloud.\n\nView all articles\n\n### Comments\n\nThere are no comments added yet.\n\n#### Let's hear your thoughts\n\nComment\n\nYour Name (Required)\n\nYour Email (Required)\n\nFor my eyes only\n\nYour Website\n\nWould you like to sign up to the news letter?\n ← Click here",
    "summary": "How do you manage a company with 50,000 employees? You need processes that give you visibility and control across every function such as technology, logistics, operations, and more. But the moment you try to create a single process to govern everyone, it stops working for anyone. One system can't cater to every team, every workflow, every context. When implemented you start seeing in-fighting, projects missing deadlines, people quitting. Compromises get made, and in my experience, it almost alwa",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 5
    }
  },
  "a4c6fcb23d752c1d2313209faaa660e4fbe8ea1f": {
    "id": "a4c6fcb23d752c1d2313209faaa660e4fbe8ea1f",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "simonwillison.net",
    "title": "Please, please, please stop using passkeys for encrypting user data",
    "url": "https://simonwillison.net/2026/Feb/27/passkeys/#atom-everything",
    "published_at": "2026-02-27T22:49:32+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Simon Willison’s Weblog\n\nSubscribe\n\n**Sponsored by:** Augment Code — Agent Orchestration. Living Specs. Your favorite agents. Build with Intent.\n\n27th February 2026 - Link Blog\n\n**Please, please, please stop using passkeys for encrypting user data** (via) Because users lose their passkeys *all the time*, and may not understand that their data has been irreversibly encrypted using them and can no longer be recovered.\n\nTim Cappalli:\n\n> To the wider identity industry: *please stop promoting and using passkeys to encrypt user data. I’m begging you. Let them be great, phishing-resistant authentication credentials*.\n\nPosted 27th February 2026 at 10:49 pm\n\n## Recent articles\n\n* I vibe coded my dream macOS presentation app - 25th February 2026\n* Writing about Agentic Engineering Patterns - 23rd February 2026\n* Adding TILs, releases, museums, tools and research to my blog - 20th February 2026\n\nThis is a **link post** by Simon Willison, posted on 27th February 2026.\n\nsecurity\n577\n\nusability\n98\n\npasskeys\n5\n\n### Monthly briefing\n\nSponsor me for **$10/month** and get a curated email digest of the month's most important LLM developments.\n\nPay me to send you less!\n\nSponsor & subscribe\n\n* Disclosures\n* Colophon\n* ©\n* 2002\n* 2003\n* 2004\n* 2005\n* 2006\n* 2007\n* 2008\n* 2009\n* 2010\n* 2011\n* 2012\n* 2013\n* 2014\n* 2015\n* 2016\n* 2017\n* 2018\n* 2019\n* 2020\n* 2021\n* 2022\n* 2023\n* 2024\n* 2025\n* 2026",
    "summary": "Please, please, please stop using passkeys for encrypting user data Because users lose their passkeys all the time , and may not understand that their data has been irreversibly encrypted using them and can no longer be recovered. Tim Cappalli: To the wider identity industry: please stop promoting and using passkeys to encrypt user data. I’m begging you. Let them be great, phishing-resistant authentication credentials . Via lobste.rs Tags: security , usability , passkeys",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "1fb32801f5c15890f9e964f568d6cda127986513": {
    "id": "1fb32801f5c15890f9e964f568d6cda127986513",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "simonwillison.net",
    "title": "An AI agent coding skeptic tries AI agent coding, in excessive detail",
    "url": "https://simonwillison.net/2026/Feb/27/ai-agent-coding-in-excessive-detail/#atom-everything",
    "published_at": "2026-02-27T20:43:41+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Simon Willison’s Weblog\n\nSubscribe\n\n**Sponsored by:** Augment Code — Agent Orchestration. Living Specs. Your favorite agents. Build with Intent.\n\n27th February 2026 - Link Blog\n\n**An AI agent coding skeptic tries AI agent coding, in excessive detail**. Another in the genre of \"OK, coding agents got good in November\" posts, this one is by Max Woolf and is very much worth your time. He describes a sequence of coding agent projects, each more ambitious than the last - starting with simple YouTube metadata scrapers and eventually evolving to this:\n\n> It would be arrogant to port Python's scikit-learn — the gold standard of data science and machine learning libraries — to Rust with all the features that implies.\n>\n> But that's unironically a good idea so I decided to try and do it anyways. With the use of agents, I am now developing `rustlearn` (extreme placeholder name), a Rust crate that implements not only the fast implementations of the standard machine learning algorithms such as logistic regression and k-means clustering, but also includes the fast implementations of the algorithms above: the same three step pipeline I describe above still works even with the more simple algorithms to beat scikit-learn's implementations.\n\nMax also captures the frustration of trying to explain how good the models have got to an existing skeptical audience:\n\n> The real annoying thing about Opus 4.6/Codex 5.3 is that it’s impossible to publicly say “Opus 4.5 (and the models that came after it) are an order of magnitude better than coding LLMs released just months before it” without sounding like an AI hype booster clickbaiting, but it’s the counterintuitive truth to my personal frustration. I have been trying to break this damn model by giving it complex tasks that would take me months to do by myself despite my coding pedigree but Opus and Codex keep doing them correctly.\n\nA throwaway remark in this post inspired me to ask Claude Code to build a Rust word cloud CLI tool, which it happily did.\n\nPosted 27th February 2026 at 8:43 pm\n\n## Recent articles\n\n* I vibe coded my dream macOS presentation app - 25th February 2026\n* Writing about Agentic Engineering Patterns - 23rd February 2026\n* Adding TILs, releases, museums, tools and research to my blog - 20th February 2026\n\nThis is a **link post** by Simon Willison, posted on 27th February 2026.\n\npython\n1230\n\nai\n1881\n\nrust\n104\n\nmax-woolf\n20\n\ngenerative-ai\n1667\n\nllms\n1632\n\nai-assisted-programming\n353\n\ncoding-agents\n165\n\nagentic-engineering\n18\n\nnovember-2025-inflection\n10\n\n### Monthly briefing\n\nSponsor me for **$10/month** and get a curated email digest of the month's most important LLM developments.\n\nPay me to send you less!\n\nSponsor & subscribe\n\n* Disclosures\n* Colophon\n* ©\n* 2002\n* 2003\n* 2004\n* 2005\n* 2006\n* 2007\n* 2008\n* 2009\n* 2010\n* 2011\n* 2012\n* 2013\n* 2014\n* 2015\n* 2016\n* 2017\n* 2018\n* 2019\n* 2020\n* 2021\n* 2022\n* 2023\n* 2024\n* 2025\n* 2026",
    "summary": "An AI agent coding skeptic tries AI agent coding, in excessive detail Another in the genre of \"OK, coding agents got good in November\" posts, this one is by Max Woolf and is very much worth your time. He describes a sequence of coding agent projects, each more ambitious than the last - starting with simple YouTube metadata scrapers and eventually evolving to this: It would be arrogant to port Python's scikit-learn — the gold standard of data science and machine learning libraries — to Rust with ",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 3
    }
  },
  "59ff6fc13e527ef6d968134631406c4303d4a0d4": {
    "id": "59ff6fc13e527ef6d968134631406c4303d4a0d4",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "simonwillison.net",
    "title": "Free Claude Max for (large project) open source maintainers",
    "url": "https://simonwillison.net/2026/Feb/27/claude-max-oss-six-months/#atom-everything",
    "published_at": "2026-02-27T18:08:22+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Simon Willison’s Weblog\n\nSubscribe\n\n**Sponsored by:** Augment Code — Agent Orchestration. Living Specs. Your favorite agents. Build with Intent.\n\n27th February 2026 - Link Blog\n\n**Free Claude Max for (large project) open source maintainers** (via) Anthropic are now offering their $200/month Claude Max 20x plan for free to open source maintainers... for six months... and you have to meet the following criteria:\n\n> * **Maintainers:** You're a primary maintainer or core team member of a public repo with 5,000+ GitHub stars *or* 1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months.\n> * **Don't quite fit the criteria** If you maintain something the ecosystem quietly depends on, apply anyway and tell us about it.\n\nAlso in the small print: \"Applications are reviewed on a rolling basis. We accept up to 10,000 contributors\".\n\nPosted 27th February 2026 at 6:08 pm\n\n## Recent articles\n\n* I vibe coded my dream macOS presentation app - 25th February 2026\n* Writing about Agentic Engineering Patterns - 23rd February 2026\n* Adding TILs, releases, museums, tools and research to my blog - 20th February 2026\n\nThis is a **link post** by Simon Willison, posted on 27th February 2026.\n\nopen-source\n291\n\nai\n1881\n\ngenerative-ai\n1667\n\nllms\n1632\n\nanthropic\n260\n\nclaude\n254\n\n### Monthly briefing\n\nSponsor me for **$10/month** and get a curated email digest of the month's most important LLM developments.\n\nPay me to send you less!\n\nSponsor & subscribe\n\n* Disclosures\n* Colophon\n* ©\n* 2002\n* 2003\n* 2004\n* 2005\n* 2006\n* 2007\n* 2008\n* 2009\n* 2010\n* 2011\n* 2012\n* 2013\n* 2014\n* 2015\n* 2016\n* 2017\n* 2018\n* 2019\n* 2020\n* 2021\n* 2022\n* 2023\n* 2024\n* 2025\n* 2026",
    "summary": "Free Claude Max for (large project) open source maintainers Anthropic are now offering their $200/month Claude Max 20x plan for free to open source maintainers... for six months... and you have to meet the following criteria: Maintainers: You're a primary maintainer or core team member of a public repo with 5,000+ GitHub stars or 1M+ monthly NPM downloads. You've made commits, releases, or PR reviews within the last 3 months. Don't quite fit the criteria If you maintain something the ecosystem q",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "4346b70b36eb159d516172460675f673064d4c83": {
    "id": "4346b70b36eb159d516172460675f673064d4c83",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "simonwillison.net",
    "title": "Unicode Explorer using binary search over fetch() HTTP range requests",
    "url": "https://simonwillison.net/2026/Feb/27/unicode-explorer/#atom-everything",
    "published_at": "2026-02-27T17:50:54+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Simon Willison’s Weblog\n\nSubscribe\n\n**Sponsored by:** Augment Code — Agent Orchestration. Living Specs. Your favorite agents. Build with Intent.\n\n27th February 2026 - Link Blog\n\n**Unicode Explorer using binary search over fetch() HTTP range requests**. Here's a little prototype I built this morning from my phone as an experiment in HTTP range requests, and a general example of using LLMs to satisfy curiosity.\n\nI've been collecting HTTP range tricks for a while now, and I decided it would be fun to build something with them myself that used binary search against a large file to do something useful.\n\nSo I brainstormed with Claude. The challenge was coming up with a use case for binary search where the data could be naturally sorted in a way that would benefit from binary search.\n\nOne of Claude's suggestions was looking up information about unicode codepoints, which means searching through many MBs of metadata.\n\nI had Claude write me a spec to feed to Claude Code - visible here - then kicked off an asynchronous research project with Claude Code for web against my simonw/research repo to turn that into working code.\n\nHere's the resulting report and code. One interesting thing I learned is that Range request tricks aren't compatible with HTTP compression because they mess with the byte offset calculations. I added `'Accept-Encoding': 'identity'` to the `fetch()` calls but this isn't actually necessary because Cloudflare and other CDNs automatically skip compression if a `content-range` header is present.\n\nI deployed the result to my tools.simonwillison.net site, after first tweaking it to query the data via range requests against a CORS-enabled 76.6MB file in an S3 bucket fronted by Cloudflare.\n\nThe demo is fun to play with - type in a single character like `ø` or a hexadecimal codepoint indicator like `1F99C` and it will binary search its way through the large file and show you the steps it takes along the way:\n\n![Animated demo of a web tool called Unicode Explore. I enter the ampersand character and hit Search. A box below shows a sequence of HTTP binary search requests made, finding in 17 steps with 3,864 bytes transferred and telling me that ampersand is U+0026 in Punctuation other, Basic Latin](https://static.simonwillison.net/static/2026/unicode-explore.gif)\n\nPosted 27th February 2026 at 5:50 pm\n\n## Recent articles\n\n* I vibe coded my dream macOS presentation app - 25th February 2026\n* Writing about Agentic Engineering Patterns - 23rd February 2026\n* Adding TILs, releases, museums, tools and research to my blog - 20th February 2026\n\nThis is a **link post** by Simon Willison, posted on 27th February 2026.\n\nalgorithms\n17\n\nhttp\n101\n\nresearch\n9\n\ntools\n50\n\nunicode\n35\n\nai\n1881\n\ngenerative-ai\n1667\n\nllms\n1632\n\nai-assisted-programming\n353\n\nvibe-coding\n74\n\nhttp-range-requests\n12\n\n### Monthly briefing\n\nSponsor me for **$10/month** and get a curated email digest of the month's most important LLM developments.\n\nPay me to send you less!\n\nSponsor & subscribe\n\n* Disclosures\n* Colophon\n* ©\n* 2002\n* 2003\n* 2004\n* 2005\n* 2006\n* 2007\n* 2008\n* 2009\n* 2010\n* 2011\n* 2012\n* 2013\n* 2014\n* 2015\n* 2016\n* 2017\n* 2018\n* 2019\n* 2020\n* 2021\n* 2022\n* 2023\n* 2024\n* 2025\n* 2026",
    "summary": "Unicode Explorer using binary search over fetch() HTTP range requests Here's a little prototype I built this morning from my phone as an experiment in HTTP range requests, and a general example of using LLMs to satisfy curiosity. I've been collecting HTTP range tricks for a while now, and I decided it would be fun to build something with them myself that used binary search against a large file to do something useful. So I brainstormed with Claude . The challenge was coming up with a use case for",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 3
    }
  },
  "7256fb26153e29c51d036129384ae5ba940372c0": {
    "id": "7256fb26153e29c51d036129384ae5ba940372c0",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "lcamtuf.substack.com",
    "title": "Approximation game",
    "url": "https://lcamtuf.substack.com/p/approximation-game",
    "published_at": "2026-02-28T02:26:51+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Approximation game\n\n### The number 22/7 and the pigeon flocks of Peter Gustav Lejeune Dirichlet\n\nFeb 28, 2026\n\n2\n\n1\n\nShare\n\nIn some of the earlier articles on this blog, we talked about the nature of real numbers and the meanings of infinity. The theory outlined in these posts is interesting but also hopelessly abstract. It’s as if we’re inventing make-believe worlds that have no discernible connection to reality.\n\nIn today’s post, let’s look at a cool counterexample: a surprising outcome of an numerical experiment that can be backed up with fairly simple proofs, but that makes sense only if you take a step back to consider the construction of real numbers and rationals.\n\nWe start by picking a real number *r*. Your job is to approximate it as closely as possible using a rational fraction *a / b* with a reasonably small denominator. Is this task easier if *r* itself is rational or irrational?\n\nBefore we dive in, we ought to specify an additional rule: to avoid trivial solutions, we’re looking for close but inexact matches. In other words, your *a / b* can’t be the same as *r*. For simplicity, we’ll also stick to positive *r* and positive fraction denominators throughout the article.\n\n### Defining “good”\n\nFor a chosen denominator *b*, it’s pretty easy to find the value of *a* that gets us the closest to the target *r*. We must consider two cases: the largest “low-side” fraction that’s still less than *r*; and the smallest “high-side” fractiongreater than *r.* Again, if there’s a rational fraction that matches *r* exactly, that solution is prohibited by the rules of the game; we need to pick one of the nearby values instead.\n\nIf you *wanted to* find an exact match, you could try *aideal = r · b*; this makes the *a / b* fraction equal to *r · b / b =* *r*. That said, *r · b* might not be an integer (or even a rational number), so even without the added constraint, the approach is usually a bust.\n\nIf we round the value up (⌈*r · b*⌉), we get a number that is equal or greater than *aideal*; if it’s greater, the difference between the two values will be less than 1. In other words, we can write the following inequality:\n\n\\(a\\_{ideal} \\leq\\lceil r · b\\rceil < a\\_{ideal} + 1\\)\n\nThis is saying that the rounded-up numbermay be equal to what we need to match *r* exactly, or it might overshoot the target, but always by less than the minimum possible increment of the numerator in the *a / b* fraction.\n\nThe result is *almost* what we need, but once more, the rules prohibit approximations that are exactly equal to *r*. The workaround is to subtract 1 from all sides of the inequality:\n\n\\(a\\_{ideal} - 1 \\leq \\lceil r · b \\rceil -1 < a\\_{ideal}\\)\n\nThe effectively tells us that the middle term — ⌈*r · b*⌉ - 1 *—* is always less than the value needed to match *r*, and the difference is never greater than a single tick of the numerator. We’re as close as we can be; the value of *a* for the optimal low-side approximation (*a / b* < *r)* is:\n\n\\(a\\_{low} = \\lceil r \\cdot b \\rceil - 1 \\)\n\nWe can follow the same thought process to find the high-side estimate (*a / b* > *r)*:\n\n\\(a\\_{high} = \\lfloor r \\cdot b \\rfloor + 1\n\\)\n\nFinally, the error (*ε*) associated with any *a / b* can be easily calculated as:\n\n\\(\\varepsilon = \\biggl|r - \\frac{a}{b}\\biggr|\\)\n\nWe have previously established that if we pick *alow* or *ahigh*, the error can’t exceed one tick of the numerator, which works out to ± 1/*b*. As a practical example, if we’re trying to approximate *r =* 2 using *b =* 5, the best inexact solutions are 9/5 = 1.8 on the low side and 11/5 = 2.2 on the high side; they both have an error of 1/*b* = 0.2*.*\n\nNext, we’ll try to examine if the error can be less. If we find any approximations that are better than the worst-case scenario — i.e., that satisfy *ε* < 1/*b* — we’re gonna call them 1-*good.*\n\nAs an inspection aid, we can also define a normalized score *s* calculated by multiplying *ε* by *b*:\n\n\\(s = \\varepsilon \\cdot b\\)\n\nThe equation keeps the maximum error at 1 regardless of the denominator we’ve chosen. By that metric, a 1-good approximation is any with a value of *s* < 1.\n\n### The rational test case\n\nNow that we have the criteria figured out, let’s take *r =* 1/4 and analyze the optimal solutions for a couple of initial values of *b:*\n\n\\(\\begin{array}{|r|c|c|c|l|}\n\\hline\n\\mathbf{b} & \\textbf{Best a/b} & \\textbf{Error (ε)} & \\textbf{Score }\\mathbf{(s)} & \\textbf{1-good?} \\\\\n\\hline\n1 & 0/1 & 1/4 & 1/4 & yes\\\\\n\\hline\n2 & 0/2 & 1/4 & 1/2 & yes\\\\\n\\hline\n3 & 1/3 & 1/12 & 1/4 & yes \\\\\n\\hline\n4 & 0/4 & 1/4 & 1 & no \\\\\n\\hline\n5 & 1/5 & 1/20 & 1/4 & yes \\\\\n\\hline\n6 & 2/6 & 1/12 & 1/2 & yes\\\\\n\\hline\n7 & 2/7 & 1/28 & 1/4 & yes\\\\\n\\hline\n8 & 1/8 & 1/8 & 1 & no\\\\\n\\hline\n9 & 2/9 & 1/36 & 1/4 & yes\\\\\n\\hline\n10 & 2/10 & 1/20 & 1/2 & yes\\\\\n\\hline\n\\end{array}\\)\n\nWe find out that many approximations are 1-good (*ε* < 1/*b*, aka *s* < 1), beating their higher-magnitude peers: for example, 1/5 is clearly better than 2/6. Nevertheless, the results are underwhelming: the values diverge from the 1/*b* baseline by unremarkably small factors that are stuck on repeat. If we plot a larger sample, we get:\n\n![](https://substackcdn.com/image/fetch/$s_!ZWwG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0009ed6d-03a4-4777-973b-afcb6457f9dd_2500x1250.png)\n\n*Approximations of r = 1/4.*\n\nIn the plot, which uses log scale, I also included a diagonal line that represents error values decreasing with the *square* of the denominator (1/*b*²). Approximations for which the error dips below this line would be markedly better than the ones that merely dip below 1/*b*. We can label these 1/*b*² solutions as *2-good*.\n\nFor rational values of *r*, we get a handful of initial approximations below the 2-good line, but we can prove that the effect can’t last. We start by rewriting *r* as a fraction of two integers: *r* = *p* / *q*. The 2-goodness criteria is *ε* <1/*b*². We’ve previously defined *ε* =| *r - a/b* | and by the rules of the game, it must be greater than zero. Putting it all together, we can write this inequality:\n\n\\(0 < \\biggl| \\frac{p}{q} - \\frac{a}{b}\\biggr| < \\frac{1}{b^2}\\)\n\nTo tidy up, we can bring the fractions in the middle to a common denominator:\n\n\\(0 < \\biggl| \\frac{b\\cdot p - a \\cdot q}{b \\cdot q}\\biggr| < \\frac{1}{b^2}\\)\n\nThe denominator is a positive number, so there’s no harm in taking it out of the absolute-value section and multiplying all sides of the inequality by it:\n\n\\(0 < \\bigl|b \\cdot p - a \\cdot q \\bigr|< \\frac{q}{b}\\)\n\nAll the variables here are integers. If *b ≥ q*, the fraction on the right is necessarily less or equal to one. That creates an issue because it implies the following:\n\n\\(0 < \\bigl|b \\cdot p - a \\cdot q \\bigr|< 1\\)\n\nThe middle section must be an integer too, so the only solution that fits the right-hand inequality is *b · p - a · q =* 0. That solution creates a contradiction on the left: 0 < 0. We conclude that there are no integer *a, b, p,* and *q* that satisfy the inequality for *b ≥ q*. If any inexact 2-good approximations of rational numbers exist, they can only exist for *b < q.* That’s where we look next.\n\nFor the next step, note that the variable *q* is the denominator of the number we’re approximating, so it is constant for a given *r.* The *b* < *q* condition can only be met by *a / b* pairs with smaller denominators. In other words, for rationals, if rule-compliant 2-good solutions exist, there’s a tight upper bound on their number — and they must be clustered near the beginning of the plot.\n\nThis is about as much as we can squeeze out of that stone: rational numbers appear to be difficult to approximate using other rationals. The only winning move is to cram more digits into *a / b*.\n\n### Approximating irrationals\n\nIf rational fractions are tough, what about irrational numbers? Surely, these are even harder to get anywhere near!… And yet — here’s the plot of *a / b* approximations for *r* = *π*:\n\n![](https://substackcdn.com/image/fetch/$s_!0TWW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec7e857-c396-4463-af74-026977e1320c_2500x1252.png)\n\n*Approximating r = π.*\n\nNote that the plot keeps dipping below the 2-good line all over the place. And there are some really nice approximations in there! The first arrow points to 22 / 7 ≈ 3.143 (*s* ≈ 0.009). The second arrow points to an even better one: 355 / 113 ≈ 3.141593 (*s* ≈ 0.00003). What’s up with that?\n\nBefore we dive in, let’s confirm that *π* is not special. Luckily for us, these patterns crop up for other irrationals too. Here’s an example for *e:*\n\n![](https://substackcdn.com/image/fetch/$s_!hIiz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827123a4-13a4-4cd3-9597-d84ccc3fadd1_2500x1252.png)\n\n*Approximating r = e.*\n\nTo understand what’s going on, we need to build a more substantial proof, although we’ll still stay within the realm of middle-school math.\n\nTo get going, we make a simple observation: given some number *r*, we can always split it into an integer part *v* and a fractional part *x* that lies in the interval of [0, 1) — i.e., 0 ≤ *x* < 1.\n\nWe can also apply this logic to any multiple of *r*. In particular, for each integer *k* from 0 to some arbitrary upper bound *K*, we can calculate *k · r* and split it to obtain a sequence of integer parts (*v*0 to *v*K) and fractional parts (*x*0 to *x*K), e.g.:\n\n\\(\\begin{align}\n0 \\cdot \\pi \\quad &\\rightarrow \\quad v\\_0 = 0, \\; x\\_0 = 0 \\\\\n1 \\cdot \\pi \\quad &\\rightarrow \\quad v\\_1 = 3, \\; x\\_1 = 0.1415\\ldots \\\\\n2 \\cdot \\pi \\quad &\\rightarrow \\quad v\\_2 = 6, \\; x\\_2 = 0.2831\\ldots \\\\\n3 \\cdot \\pi \\quad &\\rightarrow \\quad v\\_3 = 9, \\; x\\_3 = 0.4247\\ldots \\\\\n&\\ldots\n\\end{align}\\)\n\nThe sequence of fractional parts has K + 1 elements; again, each of the elements falls somewhere in the interval [0, 1).\n\nNext, we divide the [0, 1) interval into K sub-intervals (“buckets”) of equal size:\n\n\\(\\underbrace{\\biggl[0, \\frac{1}{K}\\biggr)}\\_\\textrm{bucket 1},\n\\underbrace{\\biggl[\\frac{1}{K}, \\frac{2}{K}\\biggr)}\\_\\textrm{bucket 2},\n\\quad \\ldots, \\quad\n\\underbrace{\\biggl[\\frac{K-1}{K}, \\frac{K}{K}\\biggr)}\\_\\textrm{bucket K}\\)\n\nWe have K buckets and *K +* 1 fractional parts to toss in; no matter what the values of the fractional parts may be, at least two elements will necessarily end up in the same bucket. This is the *pigeonhole principle.*\n\nAgain, the consequence is that there’s at least one pair of indices, *g* < *h,* such that *xg* and *xh* both fell into the same bucket. We don’t know anything about the underlying values, except that by the virtue of where they are, they must be spaced less than 1 / *K* apart:\n\n\\(\\bigl|x\\_h - x\\_g\\bigr| < \\frac{1}{K}\\)\n\nEach fractional part can be restated as the difference between the original multiple of *r* and the associated integer part *v*. That is to say, we can rewrite the earlier inequality as:\n\n\\(\\bigl| \\underbrace{h \\cdot r - v\\_h}\\_{x\\_h} - (\\underbrace{g \\cdot r - v\\_g}\\_{x\\_g}) \\bigr| < \\frac{1}{K}\\)\n\nNext, let’s rearrange the terms to split out expressions that form two independent integers: (*vh* - *vg*) and (*h - g*). We label these *a* and *b:*\n\n\\(\\bigl| r \\cdot \\underbrace{(h - g)}\\_b - \\underbrace{(v\\_h - v\\_g)}\\_a \\bigr| < \\frac{1}{K}\\)\n\nNote that the value of *b* is positive (because we specified *g < h*) and that it’s necessarily less or equal than *K* (because it represents the difference of indices in a list with *K* + 1 elements).\n\nThe simplified form of the equation is:\n\n\\(\\bigl| r \\cdot b - a \\bigr| < \\frac{1}{K} \\quad \\textrm{(⚑)}\\)\n\nWe’ll come back to the flag later; for now, let’s power through and divide both sides by *b*. We get:\n\n\\({\\bigl| r - \\frac{a}{b} \\bigr|}< \\frac{1}{K\\cdot b}\\)\n\nThe left-hand portion of the expression is the same as our earlier formula for the approximation error, *ε =* | *r - a / b* |*.* As for the right-hand part, recall that *b* doesn’t exceed *K.* Therefore, the expression in the denominator — K *· b —* must be equal or greater than *b²*. Putting it all together, we have established that for any real number *r,* there exists some integer *a* and *b* such that *a / b* satisfies *ε <* 1/*b²*.\n\nThis proof is known as *Dirichlet’s approximation theorem*. At first blush, it only tells us that one 2-good approximation is guaranteed for every real. In fact, that solution might not even comply with the rules of our game because it might be exact (i.e., *ε =* 0).\n\nTo take the next step — we’re getting close to the finish line! — note that the proof doesn’t put any special constraint on the value of *K.* Let’s come back to an equation flagged (⚑) several paragraphs earlier, but rewrite it for some definite *K*1. We label the resulting pair *a*1 and *b*1:\n\n\\(\\bigl| r \\cdot b\\_1 - a\\_1 \\bigr| < \\frac{1}{K\\_1} \\\\\\)\n\nThe left-hand part is equal to the approximation error (| *r - a*1 */ b*1|) multiplied by *b*1; we’ll label this *s*. For rational numbers, *s* may be equal or greater than zero. For irrational numbers, it’s always positive because subtracting *a*1 from *b*1 · *r* (*a* ∈ ℤ, *b*1 ∈ ℤ+) will always leave some fractional part.\n\nWe’re more interested in the irrational case, so let’s look at that first. We have *a*1 and *b*1 associated with *K*1 — but what if we choose a different starting value *K*2? Well, it will give us some new *a*2 and *b*2; that said, we don’t know yet if these values are different from *a*1 and *b*1.\n\nFor a moment, let’s assume that *a*2 *= a*1 and *b*2 *= b*1 for any *K*2; in essence, we’re saying that *s* is constant. However, *s* is a positive real, so it stands to reason that once *K*2 becomes large enough, the inequality must flip: *s ≥* 1/*K*2. This produces a contradiction: at that point, *a*1 and *b*1 no longer fit. In other words, it must be that for some *K*2 > *K*1*,* we need a new, distinct *a*2 / *b*2. When approximating irrational numbers, we can keep incrementing *K* to conjure as many distinct 2-good pairs as we want.\n\nBut what about the rational case where *r* can be written as *p / q?* Here, we might be starting with a non-zero *m*, but the generation process can’t continue indefinitely. To show this, we can rewrite the left-hand expression from the earlier inequality as:\n\n\\(\\biggl| r \\cdot b\\_2 - a\\_2 \\biggr| = \\biggl| \\frac{p}{q} \\cdot b\\_2 - a\\_2 \\biggr| = \\biggl| \\frac{b\\_2 \\cdot p - a\\_2 \\cdot q}{q} \\biggr| \\)\n\nThe numerator of this fraction is an integer (because so are *a*2, *b*2, *p*, and *q*). The denominator is also an integer and is constant for a given *r.* It follows that the minimum decrement for the fraction is 1/*q*. If we keep ramping up *K*, the solution will move in discrete increments toward the degenerate case of *s = 0*. At that point, *s* satisfies any 1/*K* and the generation of new 2-good pairs stops.\n\nIn other words, in our thought experi...",
    "summary": "The number 22/7 and the pigeon flock of Peter Gustav Lejeune Dirichlet.",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 13
    }
  },
  "816db41aabb69f8b3b8758fdc84913260e0e1cc1": {
    "id": "816db41aabb69f8b3b8758fdc84913260e0e1cc1",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "garymarcus.substack.com",
    "title": "Did Trump just overplay his hand?",
    "url": "https://garymarcus.substack.com/p/did-trump-just-overplay-his-hand",
    "published_at": "2026-02-27T22:55:51+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Did Trump just overplay his hand?\n\n### We will learn a lot about Silicon Valley in the upcoming days\n\n![Gary Marcus's avatar](https://substackcdn.com/image/fetch/$s_!Ka51!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fb2e48c-be2a-4db7-b68c-90300f00fd1e_1668x1456.jpeg)\n\nGary Marcus\n\nFeb 27, 2026\n\n179\n\n78\n\n29\n\nShare\n\nIn just the last 24 hours or so:\n\n• I argued that\n\n![](https://substackcdn.com/image/fetch/$s_!HyT5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad1cbe1-5b2d-4bcd-a736-867a4b1aa2b6_1157x1170.png)\n\n* The Washington Post reported that the tiff started specifically over a *nuclear weapons* scenario.\n\n  ![](https://substackcdn.com/image/fetch/$s_!7i6d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dc65b85-19cf-4f04-bae1-02977a1da77b_1505x1134.png)\n* [Mass surveillance, likely of US citizens, is also at issue.]\n* Anthropic refused to back down.\n* Sam Altman supported Anthropic on CNBC\n\n![](https://substackcdn.com/image/fetch/$s_!mdk8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd84722c2-75f4-491f-bd1e-bc201f0af27a_1142x1230.png)\n\n* Ilya Sustkever supported Anthropic:\n\n![](https://substackcdn.com/image/fetch/$s_!2xeL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F417b84bb-2d66-44cb-a15c-fdb2fcd4bf18_1152x407.png)\n\n* And then … President Trump doubled down and posted this:\n\n![](https://substackcdn.com/image/fetch/$s_!4a5-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6437a20f-1f15-48ae-8185-3de1f0e99d9b_573x903.jpeg)\n\nThis will almost certainly alienate a large part of Silicon Valley, and puts Sam Altman, and indeed the whole Valley, in a serious bind.\n\nIt will be very, very interesting to see how it all plays out.\n\n§\n\nThis tweet doesn’t seem far off the mark to me:\n\n![](https://substackcdn.com/image/fetch/$s_!8qmO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd34bf9c4-b3db-45b4-b0d3-a493995181d1_1106x309.png)\n\n§\n\nMy guess is that in time Trump will be remembered more for his AI policies than for ICE or his tariffs, and not in a good way.\n\nIf anything really bad comes of pushing premature AI too hard and too fast into to the military, he will own it.\n\nSubscribe\n\n179\n\n78\n\n29\n\nShare",
    "summary": "We will learn a lot about Silicon Valley in the upcoming days",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "f2c9f4a14c461498f0cc779df9631bf5d173832c": {
    "id": "f2c9f4a14c461498f0cc779df9631bf5d173832c",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "garymarcus.substack.com",
    "title": "Does OpenAI’s new financing make sense?",
    "url": "https://garymarcus.substack.com/p/does-openais-new-financing-make-sense",
    "published_at": "2026-02-27T20:00:32+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Does OpenAI’s new financing make sense?\n\n### I am not alone in seriously doubting it\n\n![Gary Marcus's avatar](https://substackcdn.com/image/fetch/$s_!Ka51!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8fb2e48c-be2a-4db7-b68c-90300f00fd1e_1668x1456.jpeg)\n\nGary Marcus\n\nFeb 27, 2026\n\n169\n\n61\n\n21\n\nShare\n\nThe breaking news, in case you missed it:\n\n![](https://substackcdn.com/image/fetch/$s_!uTlz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa49d8cd6-4ca3-4f8b-a8c4-ec90974533be_1650x1438.jpeg)\n\nAnd here is some analysis, reprinted by permission, from the well-known investor George Noble, a former Fidelity fund manager with a long, successful track record:\n\n![](https://substackcdn.com/image/fetch/$s_!ZLlu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a7a75be-a354-4bbc-9feb-c5c88c824235_1210x2088.jpeg)\n\n![](https://substackcdn.com/image/fetch/$s_!zgH1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb57ea6eb-7490-40e9-aa79-84716433d296_1157x2231.jpeg)\n\n![](https://substackcdn.com/image/fetch/$s_!RskB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feff3ad36-8955-4eb8-a2f2-4890f1411d7b_1193x1745.jpeg)\n\n“These aren’t arms-length investments. They’re vendor financing dressed up as venture capital.” 65% of all venture capital in one round, to one company that has never made a profit (with no immediate hopes of doing so), and no clear technical advantage. What were they thinking?!\n\n§\n\nOne thing that Noble doesn’t mention is *why* OpenAI has struggled to make a profit; in my eyes, there are three reasons, that I have emphasized here before: the product is unreliable (placing an upper bound on how much many people be willing to pay), it is expensive to operate, and they have no technical moat, which has led to price wars.\n\nThe latter (lack of a technical moat) has also meant that competitors, from Google to Anthropic, have caught up, with some Chinese companies not far behind (and ahead on price).\n\nIt is important to remember that a couple years ago the conventional wisdom (which I long cast doubt on here) was that OpenAI was unassailable. Now, by their own lights, they are in a Code Red, with Google right on their heels.\n\nThat OpenAI’s’s valuation should more or less double in a year or so when they all but squandered their former lead appears, at least to me, to be insane. Since when does an employee get a 2x raise after a bad year?\n\nEven with Nvidia and other propping them up, I hold onto my longstanding sense that there is a good chance that OpenAI will someday be seen as the WeWork of AI.\n\nThe finances just don’t make sense.\n\nSubscribe\n\n169\n\n61\n\n21\n\nShare",
    "summary": "I am not alone in seriously doubting it",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 2
    }
  },
  "a2f24ec9ed72dad33a9e81ab0434129e88d13c32": {
    "id": "a2f24ec9ed72dad33a9e81ab0434129e88d13c32",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "nesbitt.io",
    "title": "xkcd 2347",
    "url": "https://nesbitt.io/2026/02/27/xkcd-2347.html",
    "published_at": "2026-02-27T10:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "I made an interactive version of xkcd 2347, the dependency comic, where you can drag blocks out of the tower and watch everything above them collapse.\n\n![xkcd 2347 interactive game](/images/xkcd.gif)\n\nMatter.js handles the physics and Rough.js gives it the hand-drawn xkcd look. Each reload generates a different tower from a seeded PRNG that picks a taper profile, varies the block sizes and row widths, and drifts the whole thing slightly off-center as it goes up. The project names are randomly assembled from parts that sound like real packages – things like `node-flux.js` or `libcrypt-fast` or `[email protected]` – though about one in five times you’ll get an actual name like left-pad or log4j instead. Reload enough times and you might run into some unusual tower shapes, and the Konami code does what you’d hope.\n\nThe info button shows the tower’s seed, which you can share as a `?seed=` URL parameter, basically a way to say “look at this disaster” and have someone else see the exact same precarious arrangement.\n\nSome ways this could go further:\n\n* Upload an SBOM and build the tower from your actual dependency tree, with block sizes based on how many other packages depend on each one\n* Pull real dependency data from ecosyste.ms so you can see what your project’s tower looks like before you start pulling blocks out\n* Use the phone’s accelerometer to let you tilt and topple the tower\n\nSource on GitHub.",
    "summary": "An interactive version of the dependency comic.",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "ace7e34d07d6e96c50be3ad4ada8bc7d3a0e4330": {
    "id": "ace7e34d07d6e96c50be3ad4ada8bc7d3a0e4330",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "daringfireball.net",
    "title": "West Virginia’s Anti-Apple CSAM Lawsuit Would Help Child Predators Walk Free",
    "url": "https://www.techdirt.com/2026/02/25/west-virginias-anti-apple-csam-lawsuit-would-help-child-predators-walk-free/",
    "published_at": "2026-02-27T19:28:44+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "it",
    "content": "When conservatives are enabling child predators, never assume it’s inadvertent. West Virginia is a pro-pedophile shit hole.",
    "summary": "Mike Masnick, writing for Techdirt: Read that again. If West Virginia wins — if an actual court\norders Apple to start scanning iCloud for CSAM — then every\nimage flagged by those mandated scans becomes evidence obtained\nthrough a warrantless government search conducted without\nprobable cause . The Fourth Amendment’s exclusionary rule means\ndefense attorneys get to walk into court and demand that evidence\nbe thrown out. And they’ll win that motion. It’s not even a\nparticularly hard case to make. ",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "b9cb98fde34e9f9f535fce05090bf02643a9c867": {
    "id": "b9cb98fde34e9f9f535fce05090bf02643a9c867",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "daringfireball.net",
    "title": "How to Block the ‘Upgrade to Tahoe’ Alerts and System Settings Indicator",
    "url": "https://robservatory.com/block-the-upgrade-to-tahoe-alerts-and-system-settings-indicator/",
    "published_at": "2026-02-27T18:45:16+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "# Block the “Upgrade to Tahoe” alerts and System Settings indicator\n\n* Jan 17 '26Jan 17 '26\n* 12 Comments\n* Apple Universe, Mac OS X Hints, macOS, Terminal\n\nBased on some comments on my Mastodon post, this only works due to a bug in macOS 15.7.3! The 90 day period isn't supposed to be a rolling date, but 90 days from release date. So it should have no impact…but it does, so I hope Apple doesn't fix the bug.\n\nAlthough I have to have a machine running macOS Tahoe to support our customers, I personally don't like the look of Liquid Glass, nor do I like some of the functional changes Apple has made in macOS Tahoe.\n\n![](//robservatory.com/postimages/_onetime/upgrade_tahoe.jpg)So I have macOS Tahoe on my laptop, but I'm keeping my desktop Mac on macOS Sequoia for now. Which means I have the joy of seeing things like this wonderful notification on a regular basis.\n\nOr I did, until I found a way to block them, at least in 90 day chunks. Now when I open System Settings → General → Software Update, I see this:\n\n![](//robservatory.com/postimages/_onetime/upgrade_status.jpg)\n\nThe secret? Using device management profiles, which let you enforce policies on Macs in your organization, even if that \"organization\" is one Mac on your desk. One of the available policies is the ability to block activities related to major macOS updates for up to 90 days at a time (the max the policy allows), which seems like exactly what I needed.\n\nNot being anywhere near an expert on device profiles, I went looking to see what I could find, and stumbled on the Stop Tahoe Update project. The eventual goals of this project are quite impressive, but what they've done so far is exactly what I needed: A configuration profile that blocks Tahoe update activities for 90 days.\n\nI first tried to get things working by following the Read Me, but it's missing some key steps. After some fumbling about, I managed to get it working by using these modified instructions:\n\n1. Clone the repo and switch to its directory in Terminal; run the two commands as shown in the project's Read Me:\n\n   ```\n   $ git clone https://github.com/travisvn/stop-tahoe-update.git\n   $ cd stop-tahoe-update\n   ```\n2. Set all the scripts to executable (not in the instructions):\n\n   ```\n   $ chmod 755 ./scripts/*.sh\n   ```\n3. Create and insert two UUIDs into the profile (not in the instructions). To do this, use your favorite text editor to edit the file named deferral-90days.mobileconfig in the profiles folder. Look for two lines like this:\n\n   ```\n   <key>PayloadUUID</key><string>REPLACE-WITH-UUID</string>\n   ```\n\n   You need to replace that text with two distinct UUIDs; the easiest way to do that is to run uuidgen twice in Terminal, then copy and paste each UUID, replacing each REPLACE-WITH-UUID text with the UUID. Save the changes and quit the editor, unless you want to make the following optional change...\n4. **Optional step:** I didn't want to defer normal updates, just the major OS update, so I changed the Optional (set to your taste) section to look like this:\n\n   ```\n   <!-- Optional (set to your taste) -->\n         <key>forceDelayedSoftwareUpdates</key><false/>\n   ```\n\n   This way, I'll still get notifications for updates other than the major OS update, in case Apple releases anything further for macOS Sequoia. Remember to save your changes, then quit the editor.\n5. Run the script as described in the project's Read Me:\n\n   ```\n   ./scripts/install-profile.sh profiles/deferral-90days.mobileconfig\n   ```\n\n   When run, you'll see output in Terminal indicating that you're not done yet:\n\n   ```\n   Installing profile: profiles/deferral-90days.mobileconfig\n   profiles tool no longer supports installs.  Use System Settings Profiles to add configuration profiles.\n   Done. You may need to open System Settings → Privacy & Security → Profiles to approve.\n   ```\n\n   You'll also get an onscreen alert saying basically the same thing.\n6. To finish the installation, open System Settings and click on the *Profile Downloaded* entry in the sidebar. This will take you to a screen showing the profile you just added. Double-click on that profile, and a dialog appears showing the settings; here's how mine looked, reflecting the changes I made to remove minor updates from the policy:\n\n   ![](//robservatory.com/postimages/_onetime/approve_profile.jpg)\n\n   Click the Install button, which will lead you to Yet Another Dialog; again click Install and you'll finally be done. Quit and relaunch System Settings, and you should see a message like mine at the top of the Software Update panel.\n\nAs I've just done all this today, I'm not sure exactly what happens in 90 days. I imagine I may be notified that the policy has expired, or maybe I'll just see a macOS Tahoe update notification. Either way, you can reinstall the policy again by just running the install-profile.sh command again. Alternatively, and to make things much simpler, here's what I've done…\n\nI copied my modified profile (the deferral-90days.mobileconfig file in the Profiles folder) to one of my utility folders, so I could remove the repo as I won't need it any more. Then I looked at the install script, which tries to install the profile using the profiles command, and if that fails, it then opens the profile to install it. In Sequoia, you can't use profiles to install a profile, so only the open part of the command is needed.\n\nOnce I figured out I only needed to use the open command, I added a simple alias in my .zshrc configuration file:\n\n```\n# Reinstall the no-Tahoe 90 day policy\nalias notahoe='open \"/path/to/deferral-90days.mobileconfig\"; sleep 2; open \"x-apple.systempreferences:com.apple.preferences.configurationprofiles\"'\n```\n\nNow I just have to type notahoe every 90 days, and the profile will be reinstalled, and System Settings will open to the profiles panel, where a few clicks will finish activating the installed profile. We'll see how that goes in April :).\n\nI am *so* much happier now, not being interrupted with the Tahoe update notification, and not having the glaring red \"1\" on the System Settings icon.\n\n### Related Posts:\n\n* How to enable the \"Beta updates\" feature in macOS 13.4+\n* It's been quiet around here lately…\n* A full history of macOS (OS X) release dates and rates\n* A simple AppleScript to reveal System Settings' anchors\n* The macOS' version of the cp Unix command won't create links\n* Create macOS automations using a little-known app\n\n## 12 thoughts on “Block the “Upgrade to Tahoe” alerts and System Settings indicator”\n\n1. ![](https://secure.gravatar.com/avatar/b507f6a3a666aea23d0ea2281000689093e63e4a981161761690cbf9e57712d1?s=50&d=mm&r=g)\n\n   Steven Woolgar   Jan 17 '26 at 12:15 pm\n\n   Reply\n\n   Thank you..\n2. ![](https://secure.gravatar.com/avatar/b141b5d036a68422e99a284e327c56c3454768948eeec4482a1ac051c2a79df1?s=50&d=mm&r=g)\n\n   Adam Selby   Jan 17 '26 at 1:10 pm\n\n   Reply\n\n   This will only defer an update for 90 days \\*\\*from the release date\\*\\*. You can see a table of deferral dates in the [SOFA](https://sofa.macadmins.io/release-deferrals) project.\n\n   You will soon see macOS 26.0 (and 26.0.1) as 90 days have already passed since its release date. On January 31st, you will see 26.1, and so on.\n3. ![](https://secure.gravatar.com/avatar/3c26320aab4a9fd35966fe489c8ceb9a6bf9998335df97470d5ecb54664be140?s=50&d=mm&r=g)\n\n   Ronin   Jan 20 '26 at 10:11 pm\n\n   Reply\n\n   Great find Adam. If I interpret what you are saying and from personal experience:\n\n   1. I NEVER received the forced / heavy handed / Persistant Nag / Red Bubble until 26.2  \n    2. Ran the deferment today, so if I interpret this, I am good until March 11th  \n    3. Chances are good there will be a 26.2.x or 26.3 within the next 90 days, so by this logic re-running it against say 26.3 extends again. Correct?\n4. 1. ![](https://secure.gravatar.com/avatar/b141b5d036a68422e99a284e327c56c3454768948eeec4482a1ac051c2a79df1?s=50&d=mm&r=g)\n\n      Adam Selby   Jan 22 '26 at 4:37 pm\n\n      Reply\n\n      I support and manage Macs in the enterprise world, so I'm quite familiar with deferral profiles (for better or worse). Historically, there have been bugs with deferrals not being honored with the heavy handed or notification-driven upgrade reminders.\n\n      There is no deferment to run, really. It's a setting enforced by Configuration Profile, which is always set and active. Any new OS update or upgrade will be deferred for 90 days, as long as that profile is installed.\n5. ![](https://secure.gravatar.com/avatar/faf23e8d80ab433b6d1c7b6f468de6a92ba8e0ac40faf14366c5a6a5a70b0a56?s=50&d=mm&r=g)\n\n   igre   Jan 21 '26 at 2:31 pm\n\n   Reply\n\n   I NEVER received the forced /\n6. ![](https://secure.gravatar.com/avatar/faf23e8d80ab433b6d1c7b6f468de6a92ba8e0ac40faf14366c5a6a5a70b0a56?s=50&d=mm&r=g)\n\n   spiele   Jan 26 '26 at 10:26 am\n\n   Reply\n\n   I NEVER received the forced /\n7. ![](https://secure.gravatar.com/avatar/840b242864cdaa54480fe550e02d27073abeaf3d0bf125f0f1c0e42eb94088f7?s=50&d=mm&r=g)\n\n   Eli Mehler   Feb 5 '26 at 3:02 am\n\n   Reply\n\n   I just stumbled upon this website at like 5:55 am on a school day and man is it interesting this is a cool website although I dont use Mac I have a iPhone and understand the struggles I myself updated to ios 26 cause I'm a big frugiter aero fan and the UI reminds me of it but some people like the current design and that's fair apples this is pretty crappy\n8. ![](https://secure.gravatar.com/avatar/93c0599f844bdf5bc97c1246bbd12e3c2e3e5d54460d77d45dccff0b0c037edd?s=50&d=mm&r=g)\n\n   Fred   Feb 14 '26 at 2:51 pm\n\n   Reply\n\n   Thanks for turning me on to the stop-tahoe-update project!  \n    Luckily I am still on 15.7.3, but I see that as of 3 days ago Apple released 15.7.4.  \n    While I would normally embrace the security update (while avoiding Tahoe), due to the nature of this exploit (if you can call it that) being a bug in 15.7.3, I'm leery about upgrading to 15.7.4.\n\n   I ran through your steps and I can confirm that this did block the nagging to update to 26.2, but now I'm being nagged to upgrade to 15.7.4. \\*Sigh\\*\n9. ![](https://secure.gravatar.com/avatar/aab6ab7714df155c617d93a5bcd1d9fc0fac518a8e4c6ef8933bc09970dd2c45?s=50&d=mm&r=g)\n\n   DrChou   Feb 20 '26 at 8:49 am\n\n   Reply\n\n   Solved in this video;\n\n   https://www.youtube.com/watch?v=GrNJSIywjnY\n\n   No more updates or update notifications.  \n    It works and its easy.\n10. ![](https://secure.gravatar.com/avatar/8175426c2aa5c66f29108ac20c2fe52589cd41732eaabc728ab4d2c70fe57797?s=50&d=mm&r=g)\n\n    Dark Nova   Feb 25 '26 at 3:11 am\n\n    Reply\n\n    helllo i am Dark nova\n11. ![](https://secure.gravatar.com/avatar/ca871b3caae791a6a4b969195738f187bc7e993d13e98d522153a21885557fb6?s=50&d=mm&r=g)\n\n    ER   Feb 27 '26 at 12:49 pm\n\n    Reply\n\n    This is a great tutorial thanks! Saw this on Daring Fireball. Worked great for me, I only needed to make the change to the profile file to set:  \n     forceDelayedSoftwareUpdates\n\n    The install shell script already takes care of generating UUID's and finds the profile location so I only needed to run:  \n     ./scripts/install-profile.sh and then go to profiles in System Settings to activate it.\n12. ![](https://secure.gravatar.com/avatar/b45cae95167a998f8bffd158e354cf4f30182558f44e181db7a7a4d9bf7df32c?s=50&d=mm&r=g)\n\n    magic   Feb 27 '26 at 2:40 pm\n\n    Reply\n\n    Is there a way to continue to get Sequoia OS and app updates, but not see the Tahoe-specific updates that Apple uses to trick people into upgrading, like the Safari 26.3 update? Seriously what is wrong with these people?\n\n## Leave a Reply Cancel reply\n\nYour email address will not be published. Required fields are marked \\*\n\nName \\*\n\nEmail \\*\n\nWebsite\n\nComment \\*\n\nΔ",
    "summary": "Rob Griffiths, writing at The Robservatory : So I have macOS Tahoe on my laptop, but I’m keeping my desktop\nMac on macOS Sequoia for now. Which means I have the joy of\nseeing things like this wonderful notification on a regular\nbasis. Or I did, until I found a way to block them, at least in\n90 day chunks. [...] The secret? Using device management profiles , which let you\nenforce policies on Macs in your organization, even if that\n“organization” is one Mac on your desk. One of the available\npolic",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 9
    }
  },
  "1bd540468b76ff55b9fb9608d99d561b91e86f95": {
    "id": "1bd540468b76ff55b9fb9608d99d561b91e86f95",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "daringfireball.net",
    "title": "★ A Sometimes-Hidden Setting Controls What Happens When You Tap a Call in the iOS 26 Phone App",
    "url": "https://daringfireball.net/2026/02/sometimes_hidden_setting_phone_app",
    "published_at": "2026-02-27T18:12:53+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "![Daring Fireball](/graphics/logos/)\n\nBy **John Gruber**\n\n* Archive\n* The Talk Show\n* Dithering\n* Projects\n* Contact\n* Colophon\n* Feeds / Social\n* Sponsorship\n\n![Sentry](/martini/images/sentry-gradient.png)\n\nSentry — Catch, trace,  \n and fix bugs across your entire stack.\n\n# A Sometimes-Hidden Setting Controls What Happens When You Tap a Call in the iOS 26 Phone App\n\n###### Friday, 27 February 2026\n\nBack in December, Adam Engst wrote this interesting follow-up to his feature story at TidBITS a few weeks prior exploring the differences between the new Unified and old Classic interface modes for the Phone app in iOS 26. It’s also a good follow-up to my month-ago link to Engst’s original feature, as well as a continuation of my recent theme on the fundamentals of good UI design.\n\nThe gist of Engst’s follow-up is that one of the big differences between Unified and Classic modes is what happens when you tap on a row in the list of recent calls. In Classic, tapping on a row in the list will initiate a new phone call to that number. There’s a small “ⓘ” button on the right side of each row that you can tap to show the contact info for that caller. That’s the way the Phone app has always worked. In the new iOS 26 Unified mode, this behavior is reversed: tapping on the row shows the contact info for that caller, and you need to tap a small button with a phone icon on the right side of the row to immediately initiate a call.\n\nEngst really likes this aspect of the Unified view, because the old behavior made it too easy to initiate a call accidentally, just by tapping on a row in the list. I’ve made many of those accidental calls the same way, and so I prefer the new Unified behavior for the same reason. Classic’s tap-almost-anywhere-in-the-row-to-start-a-call behavior is a vestige of some decisions with the original iPhone that haven’t held up over the intervening 20 years. With the original iPhone, Apple was still stuck — correctly, probably! — in the mindset that the iPhone was first and foremost a cellular telephone, and initiating phone calls should be a primary one-tap action. No one thinks of the iPhone as primarily a telephone these days, and it just isn’t iOS-y to have an action initiate just by tapping anywhere in a row in a scrolling list. You don’t tap on an email message to reply to it. You tap a Reply button. Phone calls are particularly pernicious in this regard because the recipient is interrupted too — it’s not just an inconvenience to *you*, it’s an interruption to someone else, and thus also an embarrassment to you.\n\nHere’s where it gets weird.\n\nThere’s a preference setting in Settings → Apps → Phone for “Tap Recents to Call”. If you turn this option on, you then get the “tap anywhere in the row to call the person” behavior while using the new Unified view. *But this option only appears in the Settings app when you’re using Unified view in the Phone app.* If you switch to the Classic view in the Phone app, this option just completely disappears from the Settings app. It’s not grayed out. It’s just gone. Go read Engst’s article describing this, if you haven’t already — he has screenshots illustrating the sometimes-hidden state of this setting.\n\nI’ll wait.\n\nEngst and I discussed this at length during his appearance on The Talk Show earlier this week. Especially after talking it through with him on the show, I think I understand both what Apple was thinking, and also why their solution feels so wrong.\n\nAt first, I thought the solution was just to keep this option available all the time, whether you’re using Classic or Unified as your layout in the Phone app. Why not let users who prefer the Classic layout turn off the old “tap anywhere in the row to call the person” behavior? But on further thought, there’s a problem with this. If you just want your Phone app to keep working the way it always had, you want Classic to default to the old tap-in-row behavior too. What Apple wants to promote to users is both a new layout and a new tap-in-row behavior. So when you switch to Unified in the Phone app, Apple wants you to experience the new tap-in-row behavior too, where you need to specifically tap the small phone-icon button in the row to call the person, and tapping anywhere else in the row opens a contact details view.\n\nThere’s a conflict here. You can’t have the two views default to different row-tapping behavior if one single switch applies to both views.\n\nApple’s solution to this dilemma — to show the “Tap Recents to Call” in Settings if, and only if, Unified is the current view option in the Phone app — is lazy. And as a result, it’s quite confusing. No one expects an option like this to only appear *sometimes* in Settings. You pretty much need to understand everything I’ve written about in this article to understand why and when this option is visible. Which means almost no one who uses an iPhone is ever going to understand it. No one expects a toggle in one app (Phone) to control the visibility of a switch in another app (Settings).\n\nMy best take at a proper solution to this problem would be for the choice between Classic and Unified views to be mirrored in Settings → Apps → Phone. Show this same bit of UI, that currently is only available in the Filter menu in the Phone app, in both the Phone app *and* in Settings → Apps → Phone:\n\n![Screenshot showing the Classic/Unified choice from the iOS 26 Phone app's Filter menu.](https://daringfireball.net/misc/2026/02/phone-26-classic-or-unified.png)\n\nIf you change it in one place, the change should be reflected, immediately, in the other. It’s fine to have the same setting available both in-app and inside the Settings app.\n\nThen, in the Settings app, the “Tap Recents to Call” option could appear underneath the Classic/Unified switcher only when “Unified” is selected. Switch from Classic to Unified and the “Tap Recents to Call” switch would appear underneath. Switch from Unified to Classic and it would disappear. (Or instead of disappearing, it could gray out to indicate the option isn’t available when Classic is selected.) The descriptive text describing the option could even state that it’s an option only available with Unified.1\n\nThe confusion would be eliminated if the Classic/Unified toggle were mirrored in Settings. That would make it clear why “Tap Recents to Call” only appears when you’re using Unified — because your choice to use Unified (or Classic) would be right there.\n\n---\n\n1. Or, Apple could offer separate “Tap Recents to Call” options for both Classic and Unified. With Classic, it would default to On (the default behavior since 2007), and with Unified, default to Off (the idiomatically correct behavior for modern iOS). In that case, the descriptive text for the option would \\*need\\* to explain that it’s a separate setting for each layout, or perhaps the toggle labels could be “Tap Recents to Call in Classic” and “Tap Recents to Call in Unified”. But somehow it would need to be made clear that they’re separate switches. But this is already getting more complicated. I think it’d be simpler to just keep the classic tap-in-row behavior with the Classic layout, and offer this setting only when using the Unified view. ↩︎\n\n|  |  |\n| --- | --- |\n| **Previous:** | My 2025 Apple Report Card |\n\nDisplay Preferences",
    "summary": "Apple’s solution to this dilemma — to show the “Tap Recents to Call” in Settings if, and only if, Unified is the current view option in the Phone app — is lazy. And as a result, it’s quite confusing.",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 6
    }
  },
  "33a6d36907f565c0be4c9e308e3397ef686929e6": {
    "id": "33a6d36907f565c0be4c9e308e3397ef686929e6",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "daringfireball.net",
    "title": "TUDUMB",
    "url": "https://spyglass.org/netflix-warner-bros-paramount-deal/",
    "published_at": "2026-02-27T16:32:49+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "Congratulations on saying the biggest number, Paramount. $111B for a company that a year ago had a market cap of around $20B. For a company that *shrunk* in their most recent quarter, and in fact, *for the entire year*, with revenue down 5% to $37.3B. Paramount may not be buying the Titanic, but only because they already own that IP.\n\nAt the same time, kudos to Netflix for showing true discipline. Ted Sarandos kept insisting they would, but that's obviously far easier said than done when you've already talked yourself into a deal which you thought you had \"won\". They undoubtedly anticipated somewhat of a circus when they swooped in and stole it from under the nose of Paramount – the original bidder, remember – in December, but it turned into a full-on clown show.1 It turned into a battle against not just Paramount (as expected), but also politicians (to be expected), and Wall Street (probably should have been expected). Perhaps the real wild card though was the hatred from Hollywood itself (more on this in a minute). So it was clearly better to swallow that pride and walk away with a nearly $3B check of pure profit.\n\nIn a way, not bad for a quarter's worth of ~~work~~ distraction. It's almost exactly what Netflix made in *actual* profit last quarter.\n\nPredictably, investors *love* this move. Netflix's stock has popped nearly 10% in after-hours trading. The share price had been ground down almost 25% since the deal was announced. The message was clear: you're the present and future of entertainment, why are you putting this albatross around your neck? The *century-old* *past* of entertainment stuck in perpetual decline?\n\nAt the highest level, it's why Netflix's deal was a surprise in the first place. Sure, you take a look at the deal, because why not? But nothing in Netflix's history suggested that they would take this too seriously. But Ted Sarandos surprised us. Clearly he saw a path to take such a storied legacy and shift it into the future. Netflix has proven their worth as an IP accelerant, what if they bought perhaps the best library available? It's an interesting idea, though I'm still not sure it made any business sense at $83B. At $100+B – the amount Netflix probably would have had to counter with (with a discount for the TV assets they wouldn't be buying, which Paramount will be buying) – I mean...\n\nOf course, Netflix could have absorbed such a cost. It's a $400B company (well, before this deal, anyway) – *double* Disney! Paramount Skydance? They're worth $11B. Yes, they're paying almost exactly $100B more than they're worth for WBD. Yes, it's looney. But really, it's leverage.\n\nTo be clear, Netflix was going to pay for the deal with debt too, but they have a clear path to repay such debts. They have a great, growing business. They don't require the backstop of one of the world's richest men, who just so happens to be the father of the CEO. How on Earth is Paramount going to pay down this debt? I'm tempted to turn to another bit of Paramount IP for the answer:\n\n1. Step one  \n2. Step two  \n3. ????  \n4. PROFIT!!!\n\nOr maybe David Ellison should start an AI company, raise billions, then merge it with Paramount. I mean, this has worked in the past for deals that make absolutely no sense on paper!\n\nBut really, the answer will undoubtedly be a combination of huge cuts – \"synergies\" – mixed with the hope that rates keep going down so this can all be constantly refinanced with the buck being passed around until they figure out a way to spin out some assets and burden them with the debt. You know, just like David Zaslav was about to do!\n\nI'm being harsh. The truth is that this is the deal that I thought Paramount *should* do! As Skydance was in the midst of acquiring National Amusements, and thus, Paramount, I wrote the following in July 2024:\n\n> There's been a lot of talk amidst the Paramount dealings that WBD might be a good home/partner. What if, once the Skydance/Paramount deal is closed, \\*they actually buy WBD\\*? Yes, there are debt issues, but a year from now, hopefully WBD head David Zaslav will have a better answer and path there. Ellison has spoken a few times about Paramount+ in particular. Most assume they'll either spin it off or merge it with another player, like WBD's Max or Comcast's Peacock. And perhaps they will. But again, I'm not sure they shouldn't just buy \\*all of\\* WBD to bulk up into one of the major players themselves.\n\nWell, they took my advice. And just over a year later, with their deal finally closed, they made their bid. But that bid was $19/share – and a mixture of cash and stock. That would have valued WBD at just below $50B (before their own aforementioned sizable debt was taken into account). After *eight* more offers from Paramount, and yes, the one from Netflix (including switching their own offer from cash/stock to all cash), here we are.\n\nAnyone who took issue with David Zaslav's pay package should apologize immediately. He may not have been great at running WBD's actual business, but the financial engineering required here to turn $50B into $80B into $111B in just a few short months – again, while the core business *declined* – is truly something.\n\nBut again, while this deal makes no sense financially, it does feel like Paramount needed it. For Netflix, Warner Bros was a nice-to-have. For Paramount, it was existential. Without the Warner Bros studio, which, before a last-minute holiday surge by Disney was the number one movie studio for much of last year, Paramount was the distant fifth place player in a group of five. Adding WB vaults them close to or at the top with Disney.\n\nMeanwhile, in streaming, Paramount+ is also the fifth place player, but there, it could be worse: they could be Peacock. Still, despite some decent numbers – thanks Taylor Sheridan (who subsequently bailed) and the NFL – they were far behind the \"major\" players. Including, yes, HBO Max. This deal vaults them near the top there too. Ahead of Disney but behind... Netflix.\n\nSo yeah, you can see why Paramount felt the need to win this deal, no matter the cost. Without it, they were a sub-scale player. With it, they're a real player.\n\nOf course, it's in a game stuck in secular decline. Disney makes most of their money from the theme parks and cruises, not the movies. There's a reason why Parks chief Josh D'Amaro is the new CEO. Yes, the IP fuels it and keeps the flywheel going, but even Disney has to manage the fact that the actual movie business is simply not a great one anymore.\n\nYou'd think Hollywood would recognize this. But it's hard when their jobs literally rely on them not recognizing it. They're blinded by box office results that exist in a magical realm where inflation doesn't seem to. If we look at tickets sold – butts in seats – the situation looks dire. And it looks even worse if you put it through the lens of per capita moviegoing in the US over time.\n\nNetflix carved a new path forward in the form of streaming. No, it's not as good of a business as the heyday of movie theaters – or even DVD sales – but it clearly works for Netflix. They're on their way to becoming the first $1T media company. Again, they thought Warner Bros' IP could have accelerated that, but now they'll find another way. One big question: without the need for Sarandos' endless promises of doing proper theatrical releases and windows, will Netflix still go down this path? I still think they will – unless Sarandos decides the path forward with Hollywood is more scorched Earth in light of their reaction to his deal.\n\nI wouldn't be shocked if Netflix goes the other way: aiming to show Hollywood what they missed by moving to dominate the box office themselves. Then again, I predicted this move long before this actual deal.\n\nPerhaps it's simply, \"They won. We lost. Next.\"2\n\nHere's the thing: Hollywood absolutely shot themselves in the foot here. They thought the Netflix/Warner deal signaled the end of their industry, when really it showcased the best possible path forward. To be fair, it's not like the industry loves this Paramount deal either, but what they wanted was *no deal*. For Warner Bros to continue on as it was, forever. That was simply not tenable and not an option. If Netflix was a path to growth, and Paramount is a path to slower decay, the status quo would have been a quicker collapse under the burden of steady, managed decline.\n\nI'm not trying to be a dick, I'm trying to paint a realistic picture. The only studio that can survive on its own is the one that has for the past century: Disney. And again, that's thanks to their other businesses propping up the studio. For a long while this was cable. Now it's the aforementioned parks.3 There's a reason why every other studio has spent much of the past 100 years being passed around various conglomerates like trading cards. These are not great businesses! Certainly not in our modern age. And when a modern age player came calling, Hollywood freaked the fuck out and threw a tantrum until they walked away. Nice work.\n\nI'll end by once again quoting some Warner Bros IP, fittingly from the fictional media mogul I kicked off by quoting. \"Money wins.\"\n\nBut really, it's more like: \"Debt wins.\" Good luck.\n\n*****Disclosure:***** **I own a relatively small amount of shares in Netflix and have for years (****though not as long as I should have****), for the reasons outlined above. As I've said throughout with these disclosures, it's probably better for the stock in the short term to \\*not\\* do this deal. And here we are.**\n\n👇\n\n****Previously, on**** *****Spyglass*********...****\n\nHollywood Cuts Off Its Future to Spite Its Present\n\nNetflix is obviously the best path forward for Warner Bros, you fools…\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1045.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/RIP_20241028_01742_R-H-2026-2.webp)\n\nOh No, a Tech Company is Buying a Movie Studio\n\nThis is the end of Hollywood? Come on.\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1044.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/Gemini_Generated_Image_wi3l0nwi3l0nwi3l-13.png)\n\nThe $1T Media Company\n\nNetflix has owned Hollywood, and aims to keep doing so…\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1051.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/mgs22_draw_the_netflix_logo_on_a_rocketship_--ar_169_--profil_c0ae6366-d703-446e-8afe-966bfa73af59_2-1-3.png)\n\nThe Grand Netflix Hollywood Unification Theory\n\nWarner Bros/HBO is phase one of Netflix’s bigger play here…\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1047.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/mgs_the_Netflix_logo_surrounded_by_planets_and_the_universe_1_2782a891-dd3a-46f6-838e-dd86a9474f9f_0-11.png)\n\nThe Albanian Army Closes in on Warner Bros\n\nIn a stunning turn, Netflix enters pole position to take over Warner Bros and HBO…\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1048.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/Gemini_Generated_Image_lsa28wlsa28wlsa2-8.png)\n\nParamount Skydance’s Blockbuster Bid for Warner Bros Discovery\n\nOne good idea, so many names…\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1049.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/wbmount2-6.png)\n\nHow to Scale Paramount\n\nCan Skydance finally, actually bridge Silicon Valley and Hollywood?\n\n![](https://spyglass.org/content/images/icon/Spyglass-Rings-----Multi-1050.png)SpyglassM.G. Siegler\n\n![](https://spyglass.org/content/images/thumbnail/mgs22_a_man_scaling_the_paramount_logo_18900cfa-a6b8-4cb4-afc3-d37bbd2ac4c2_1-3-3.png)\n\n---\n\n1 Strange how this keeps happening with Skydance deals? Also, I'm not really going to delve into the political aspects here, but I very much look forward to future reporting on that particular aspect, which sure seems to have a strong waft of a bunch of bullshit. ↩\n  \n  \n2 Though \"next\" remains figuring out a way to combat YouTube. You know, the *real* competition here... To that end, probably not crazy to think that Netflix may be able to buy at least some of these assets – at far more firesale prices! – in a few years... ↩\n\n#netflix\n\n#wbd\n\n#paramount\n\n#skydance\n\n#hollywood\n\n#movie theaters\n\n#money\n\n#david ellison\n\n#david zaslav\n\n#ted sarandos\n\n#entertainment\n\n#tech\n\n#streaming",
    "summary": "MG Siegler, writing at Spyglass: Of course, Netflix could have absorbed such a cost. It’s a $400B\ncompany (well, before this deal, anyway) — double Disney!\nParamount Skydance? They’re worth $11B. Yes, they’re paying almost\nexactly $100B more than they’re worth for WBD. Yes, it’s looney.\nBut really, it’s leverage. To be clear, Netflix was going to pay for the deal with debt too,\nbut they have a clear path to repay such debts. They have a great,\ngrowing business. They don’t require the backstop of",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 10
    }
  },
  "f244dc0eff622a3577efcc8dc1fe28ed0ec981fd": {
    "id": "f244dc0eff622a3577efcc8dc1fe28ed0ec981fd",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "daringfireball.net",
    "title": "Block Lays Off 4,000 (of 10,000) Employees",
    "url": "https://www.cnbc.com/2026/02/26/block-laying-off-about-4000-employees-nearly-half-of-its-workforce.html",
    "published_at": "2026-02-27T15:33:15+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "Skip Navigation\n\nMarkets\n\nBusiness\n\nInvesting\n\nTech\n\nPolitics\n\nVideo\n\nWatchlist\n\nInvesting Club\n\n![Join IC](https://static-redesign.cnbcfm.com/dist/93743f20be95b721880f.svg)\n\nPRO\n\n![Join Pro](https://static-redesign.cnbcfm.com/dist/69ae09b80acd376e9c97.svg)\n\nLivestream\n\nMenu\n\nKey Points\n\n* Block said Thursday it's laying off more than 4,000 employees, or about half of its head count.\n* Shares of the payment company skyrocketed as much as 24% in extended trading. It was last seen up nearly 18% in Friday's premarket.\n* Block's CFO said the job cuts would enable it \"to move faster with smaller, highly talented teams using AI to automate more work.\"\n\nIn this article\n\n* XYZ\n\nFollow your favorite stocksCREATE FREE ACCOUNT\n\n![Block shares jump as CEO Jack Dorsey links major job cuts to AI](https://image.cnbcfm.com/api/v1/image/108271047-1772208637538-1772208466-44257520606-hd.jpg?v=1772208658&w=750&h=422&vtcrop=y)\n\nwatch now\n\nVIDEO1:4801:48\n\nBlock shares jump as CEO Jack Dorsey links major job cuts to AI\n\nMoney Movers\n\nBlock said Thursday it's laying off more than 4,000 employees, or about half of its head count. The stock skyrocketed as much as 24% in extended trading.\n\n\"Today we shared a difficult decision with our team,\" Jack Dorsey, Block's co-founder and CEO, wrote in a letter to shareholders. \"We're reducing Block by nearly half, from over 10,000 people to just under 6,000, which means that over 4,000 people are being asked to leave or entering into consultation.\"\n\nBlock CFO Amrita Ahuja said the job cuts will position the company \"for our next phase of long term growth.\"\n\n\"We are choosing to shift how we operate at a time when our business is accelerating and we see an opportunity to move faster with smaller, highly talented teams using AI to automate more work,\" Ahuja wrote.\n\nBlock was last up nearly 18% in premarket trading Friday.\n\nDorsey said he expects other companies to similarly overhaul their workforces as they see more efficiency gains from \"intelligence tools.\"\n\n## Read more CNBC tech news\n\n* Trump administration blacklists Anthropic as AI firm refuses Pentagon demands\n* How Amazon's massive stake in OpenAI could boost its AI and cloud businesses\n* Block laying off more than 4,000 employees, nearly half of its workforce\n* Nintendo bets more than ever on franchises like Mario to drive Switch 2 sales\n\n\"Within the next year, I believe the majority of companies will reach the same conclusion and make similar structural changes,\" Dorsey said. \"I'd rather get there honestly and on our own terms than be forced into it reactively.\"\n\nOther companies like Pinterest, CrowdStrike and Chegg have recently announced job cuts and directly attributed the layoffs to AI reshaping their workforces.\n\nIn an X post, Dorsey said he was faced with the choice of laying off staffers over several months or years \"as this shift plays out,\" or to \"act on it now.\"\n\n\"I chose the latter,\" Dorsey wrote. \"Repeated rounds of cuts are destructive to morale, to focus, and to the trust that customers and shareholders place in our ability to lead.\"\n\nThe company had 10,205 employees worldwide as of Dec. 31, 2025, according to its annual filing.\n\nBlock announced the layoffs in conjunction with its fourth-quarter earnings results.\n\n![Block shares pop more than 20%, announces plan to reduce workforce by almost half](https://image.cnbcfm.com/api/v1/image/108270760-1772165294632-1772140589-44243148443-hd.jpg?v=1772165294&w=750&h=422&vtcrop=y)\n\nwatch now\n\nVIDEO1:0001:00\n\nBlock shares pop more than 20%, announces plan to reduce workforce by almost half\n\nClosing Bell: Overtime\n\nThe payments company reported adjusted earnings per share of 65 cents on revenue of $6.25 billion, while analysts estimated 65 cents per share and $6.24 billion, according to LSEG.\n\nGross profit increased 24% from a year earlier to $2.87 billion.\n\nFor the full year, the company said it sees adjusted earnings per share of $3.66. Analysts anticipated $3.22 per share, according to LSEG.\n\nAs a result of the workforce reduction, the company expects to incur charges of approximately $450 million to $500 million, consisting primarily of severance payments, employee benefits and noncash expenses related to share vesting, according to a securities filing.\n\nBlock expects most of the restructuring charges to be incurred in the first quarter.\n\nStock Chart IconStock chart icon\n\n![hide content](https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg)\n\nBlock year-to-date stock chart.",
    "summary": "CNBC: Block said Thursday it’s laying off more than 4,000 employees, or\nabout half of its head count. The stock skyrocketed as much as 24%\nin extended trading. “Today we shared a difficult decision with our team,” Jack\nDorsey, Block’s co-founder and CEO, wrote in a letter to\nshareholders . “We’re reducing Block by nearly half, from\nover 10,000 people to just under 6,000, which means that over\n4,000 people are being asked to leave or entering into\nconsultation.” [...] Other companies like Pintere",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 3
    }
  },
  "1a62c3e8f70d3f9049398e39c941fe191fbfecb7": {
    "id": "1a62c3e8f70d3f9049398e39c941fe191fbfecb7",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "blog.jim-nielsen.com",
    "title": "Computers and the Internet: A Two-Edged Sword",
    "url": "https://blog.jim-nielsen.com/2026/two-edged-sword-of-computers-and-internet/",
    "published_at": "2026-02-27T19:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "Dave Rupert articulated something in “Priority of idle hands” that’s been growing in my subconscious for years:\n> I had a small, intrusive realization the other day that computers and the internet are probably bad for me […] This is hard to accept because a lot of my work, hobbies, education, entertainment, news, communities, and curiosities are all on the internet. I love the internet, it’s a big part of who I am today\n\nHard same. I love computers and the internet. Always have. I feel lucky to have grown up in the late 90’s / early 00’s where I was exposed to the fascination, excitement, and imagination of PCs, the internet, and then “mobile”. What a time to make websites!\n\nSimultaneously, I’ve seen how computers and the internet are a two-edged sword for me: I’ve cut out many great opportunities with them, but I’ve also cut myself a lot (and continue to).\n\nPer Dave’s comments, I have this feeling somewhere inside of me that the internet and computers don’t necessarily align in support my own, personal perspective of what a life well lived is *for me*. My excitement and draw to them also often leave me with a feeling of “I took that too far.” I still haven’t figured out a completely healthy balance (but I’m also doing ok).\n\nDave comes up with a priority of constituencies to deal with his own realization. I like his. Might steal it. But I also think I need to adapt it, make it my own — but I don’t know what that looks like yet.\n\nTo be honest, I don't think I was ready to confront any of this but reading Dave’s blog forced it out of my subconscious and into the open, so now I gotta deal.\n\nThanks Dave.",
    "summary": "Dave Rupert articulated something in “Priority of idle hands” that’s been growing in my subconscious for years: I had a small, intrusive realization the other day that computers and the internet are probably bad for me […] This is hard to accept because a lot of my work, hobbies, education, entertainment, news, communities, and curiosities are all on the internet. I love the internet, it’s a big part of who I am today Hard same. I love computers and the internet. Always have. I feel lucky to hav",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 1
    }
  },
  "736d7725f3e6d3b1d6b36af8cb53f16ea12d8b64": {
    "id": "736d7725f3e6d3b1d6b36af8cb53f16ea12d8b64",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "wheresyoured.at",
    "title": "Premium: The Hater's Guide to Private Equity",
    "url": "https://www.wheresyoured.at/hatersguide-pe/",
    "published_at": "2026-02-27T17:07:32+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "We have a global intelligence crisis, in that a lot of people are being really fucking stupid.\n\nAs I discussed in this week’s free piece, alleged financial analyst Citrini Research put out a truly awful screed called the “2028 Global Intelligence Crisis” — a slop-filled scare-fiction written and framed with the authority of deeply-founded analysis, so much so that it caused a global selloff in stocks.\n\nAt 7,000 words, you’d expect the piece to have some sort of argument or base in reality, but what it actually says is that “AI will get so cheap that it will replace everything, and then most white collar people won’t have jobs, and then they won’t be able to pay their mortgages, also AI will cause private equity to collapse because AI will write all software.”\n\nThis piece is written specifically to spook \\*and\\* ingratiate anyone involved in the financial markets with the idea that their investments are bad but investing in AI companies is good, and also that if they don't get behind whatever this piece is about (which is unclear!), they'll be subject to a horrifying future where the government creates a subsidy generated by a tax on AI inference (seriously). And, most damningly, its most important points about HOW this all happens are single sentences that read \"and then AI becomes more powerful and cheaper too and runs on a device.\"\n\nPart of the argument is that AI agents will use cryptocurrency to replace MasterCard and Visa. It’s dogshit. I’m shocked that anybody took it seriously.\n\nThe fact this moved markets should suggest that we have a fundamentally flawed financial system — and here’s an annotated version with my own comments.\n\nThis is the second time our markets have been thrown into the shitter based on AI booster hype. A mere week and a half ago, a software sell-off began because of the completely fanciful and imaginary idea that AI *would now write all software*.\n\nI really want to be explicit here: AI does not threaten the majority of SaaS businesses, and they are jumping at ghost stories.\n\nIf I am correct, those dumping software stocks believe that AI will replace these businesses *because people will be able to code their own software solutions.* This is an intellectually bankrupt position, one that shows an alarming (and common) misunderstanding of very basic concepts. It is not just a matter of “enough prompts until it does this” — good (or even functional!) software engineering is technical, infrastructural, and philosophical, and the thing you are “automating” is not just the code that makes a thing run.\n\nLet's start with the simplest, and least-technical way of putting it: even in the best-case scenario, you do not just type \"Build Be A Salesforce Competitor\" and it erupts, fully-formed, from your Terminal window. It is not capable of *building it,* but even if it *were,* it would need to actually be on a cloud hosting platform, and have all manner of actual customer data entered into it. Building software is not writing code and then hitting enter and a website appears, requiring all manner of infrastructural things (such as \"how does a customer access it in a consistent and reliable way,\" \"how do I make sure that this can handle a lot of people at once,\" and \"is it quick to access,\" with the more-complex database systems requiring entirely separate subscriptions *just to keep them connecting*).\n\nSoftware is a tremendous pain in the ass. You write code, then you have to make sure the code actually runs, and that code needs to run in some cases on specific hardware, and that hardware needs to be set up right, and some things are written in different languages, and those languages sometimes use more memory or less memory and if you give them the wrong amounts or forget to close the door in your code on something everything breaks, sometimes costing you money or introducing security vulnerabilities.\n\nIn any case, even for experienced, well-versed software engineers, maintaining software that involves any kind of customer data requires significant investments in compliance, including things like SOC-2 audits if the customer itself ever has to interact with the system, as well as massive investments in security.\n\nAnd yet, the myth that LLMs are an existential threat to existing software companies has taken root in the market, sending the share prices of the legacy incumbents tumbling. A great example would be SAP, down 10% in the last month.\n\nSAP makes ERP (Enterprise Resource Planning, which I wrote about in the Hater's Guide To Oracle) software, and has been affected by the sell-off. SAP is also a massive, complex, resource-intensive database-driven system that involves things like accounting, provisioning and HR, and is so heinously complex that you often have to pay SAP just to make it function (if you're lucky it might even do so). If you were to build this kind of system yourself, even with \"the magic of Claude Code\" (which I will get to shortly), it would be an incredible technological, infrastructural and ***legal*** undertaking.\n\nMost software is like this. I’d say all software that people rely on is like this. I am begging with you, pleading with you to think about how much you trust the software that’s on every single thing you use, and what you do when a piece of software stops working, and how you feel about the company that does that. If your money or personal information touches it, they’ve had to go through all sorts of shit that doesn’t involve the code to bring you the software.\n\n> **Sidenote:** I want to be clear that there is nothing *good* about this. To quote a friend of mine — an editor at a large tech publication — “Oracle is a lawfirm with a software company attached.” SaaS companies regularly get by through scurrilous legal means and bullshit contracts, and their features are, in many cases, only as good as they need to be. Regardless, my point is that you will not just “make your own software.”\n\nAny company of a reasonable size would likely be committing hundreds of thousands if not millions of dollars of legal and accounting fees to make sure it worked, engineers would have to be hired to maintain it, and you, as the sole customer of this massive ERP system, would have to build **every single new feature and integration you want.** Then you'd have to keep it running, this massive thing that involves, in many cases, *tons of personally identifiable information.* You'd also need to make sure, without fail, that this system that involves money was aware of any and all currencies and how they fluctuate, because that is now your problem. Mess up that part and your system of record could massively over or underestimate your revenue or inventory, which could destroy your business.\n\nIf that happens, you won't have anyone to sue. When bugs happen, you'll have someone who's job it is to fix it that you can fire, but replacing them will mean finding a new person to fix the mess that another guy made.\n\nAnd then we get to the fact that building stuff with Claude Code is not that straightforward. Every example you've read about somebody being amazed by it has built a toy app or website that's very similar to many open source projects or website templates that Anthropic trained its training data on.\n\nEvery single piece of SaaS anyone pays for is paying for both access to the product and a transfer of the inherent risk or chaos of running software that involves people or money. Claude Code does not actually build unique software. You can say \"create me a CRM,\" but whatever CRM it pops out will not magically jump onto Amazon Web Services, nor will it magically be efficient, or functional, or compliant, or secure, nor will it be differentiated at all from, I assume, the open source or publicly-available SaaS it was trained on. You really still need engineers, if not *more* of them than you had before.\n\nIt might tell you it's completely compliant and that it will run like a hot knife through butter — but LLMs don’t know anything, and you cannot be sure Claude is telling the truth as a result. Is your argument that you’d still have a team of engineers (so they know what the outputs mean), but they’d be working on replacing your SaaS subscription? You’re basically becoming a startup with none of the benefits.\n\nTo quote Nik Suresh, an incredibly well-credentialed and respected software engineer (author of I Will Fucking Piledrive You If You Mention AI Again), “...for some engineers, [Claude Code] is a great way to solve certain, tedious problems more quickly, and the responsible ones understand you have to read most of the output, which takes an appreciable fraction of the time it would take to write the code in many cases. Claude doesn't write terrible code all the time, it's actually good for many cases because many cases are boring. You just have to read all of it if you aren't a fucking moron because it periodically makes company-ending decisions.”\n\nJust so you know, “company-ending decisions” could start with your vibe-coded Stripe clone leaking user credit card numbers or social security numbers because you asked it to “just handle all the compliance stuff.” Even if you have very talented engineers, are those engineers talented in the specifics of, say, healthcare data or finance? They’re going to need to be to make sure Claude doesn’t do anything *stupid*!\n\n# The Intelligence Crisis In Private Investing and Private Equity\n\nSo, despite all of this being *very obvious*, it’s clear that the markets and an alarming number of people in the media *simply do not know what they are talking about.* The “AI replaces software” story is literally “Anthropic has released a product and now the resulting industry is selling off,” such as when it launched a cybersecurity tool that could check for vulnerabilities (a product that has existed in some form for nearly a decade) causing a sell-off in cybersecurity stocks like Crowdstrike — you know, the one that had a faulty bit of code cause a global cybersecurity incident that lost the Fortune 500 billions, and led to Delta Air Lines suspending over 1,200 flights over six long days of disruption.\n\nThere is no rational basis for anything about this sell-off other than that our financial media and markets do not appear to understand the very basic things about the stuff they invest in. Software may *seem* complex, but (especially in these cases) it’s really quite simple: investors are conflating “an AI model can spit out code” with “an AI model can create the entire experience of what we know as “software,” or is close enough that we have to start freaking out.”\n\nThis is thanks to the intentionally-deceptive marketing pedalled by Anthropic and validated by the media. In a piece from September 2025, Bloomberg reported that Claude Sonnet 4.5 could “code on its own for up to 30 hours straight,”  a statement directly from Anthropic repeated by other outlets that added that it did so “on complex, multi-step tasks,” none of which were explained. The Verge, however, added that apparently Anthropic “coded a chat app akin to Slack or Teams,” **and no, you can’t see it, or know anything about how much it costs or its functionality.** Does it run? Is it useful? Does it work in any way? What does it look like? We have **absolutely no proof this happened other than them saying it, but because the media repeated it it’s now a fact.**\n\nPerhaps it’s not a particularly novel statement, but it’s becoming kind of obvious that *maybe the people with the money don’t actually know what they’re doing, which will eventually become a problem when they all invest in the wrong thing for the wrong reasons.*\n\nSaaS (Software as a Service, which almost always refers to business software) stocks became a hot commodity because they were perpetual growth machines with giant sales teams that existed *only to make numbers go up,* leading to a flurry of investment based on the assumption that *all numbers will always increase forever, and every market is as giant as we want.* Not profitable? No problem! You just had to show growth.\n\nIt was easy to raise money because everybody saw a big, obvious path to liquidity, either from selling to a big firm or taking the company public…\n\n…in theory.\n\n# How Private Equity Created A Pump-And-Dump Crisis In Software By Assuming Everything Would Grow Forever — And Everything Broke In 2021\n\nPer Victor Basta, between 2014 and 2017, the number of VC rounds in technology companies halved with a much smaller drop in funding, adding that a big part was the collapse of companies describing themselves as SaaS, which dropped by 40% in the same period. In a 2016 chat with VC David Yuan, Gainsight CEO Nick Mehta added that “the bar got higher and weights shifted in the public markets,” citing that profitability was now becoming more important to investors.\n\nPer Mehta, one savior had arrived — Private Equity, with Thoma Bravo buying Blue Coat Systems in 2011 for $1.3 billion (which had been backed by a Canadian teacher’s pension fund!), Vista Equity buying Tibco for $4.3 billion in 2014, and Permira Advisers (along with the Canadian Pension Plan Investment Board) buying Informatica for $5.3 billion (with participation from both Salesforce and Microsoft) in 2015, 16 years after its first IPO. In each case, these firms were purchased using debt that immediately gets dumped onto the company’s balance sheet, known as a leveraged buyout.\n\nIn simple terms, you buy a company with money that the company you just bought has to pay off. The company in question also has to grow like gangbusters to keep up with both that debt and the private equity firm’s expectations. And instead of being an investor with a board seat who can yell at the CEO, *it’s quite literally your company, and you can do whatever you want with (or to) it.*\n\nYuan added that the size of these deals made the acquisitions problematic, as did their debt-filled:\n\n> Recent SaaS PE deals are different. At more than six times revenues, unless you can increase EBITDA margins to over 40%, it’s hard to get your arms around the effective EBITDA multiple. It seems the new breed of PE buyer is taking a bet that SaaS companies will exit on revenue multiples and show rapid growth over many years. Both are arguably new bets for private equity. It’s not about financial or cost engineering. They are starting to look a bit more like us in the growth investing industry and taking a bet on category leadership and growth  \n>   \n> …  \n>   \n> So while revenue multiples are accepted, they are viewed as risky by private equity. Take Salesforce.com, the bellwether of SaaS. Over the last 10 years, it’s traded below 2 times next-twelve-months (NTM) revenues and over 10 times NTM revenues. Even in the past 12 months, it’s traded as low as 4.7 times NTM multiples and as high as close to 9 times NTM multiples. In this example, if the private equity firm paid 9 times NTM revenues and multiples traded down to 4.7 times NTM, their $300 millio...",
    "summary": "We have a global intelligence crisis, in that a lot of people are being really fucking stupid. As I discussed in this week’s free piece , alleged financial analyst Citrini Research put out a truly awful screed called the “2028 Global Intelligence Crisis” — a slop-filled scare-fiction",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 13
    }
  },
  "2f589326f2c06f2369b341280be468656e6e225a": {
    "id": "2f589326f2c06f2369b341280be468656e6e225a",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "minimaxir.com",
    "title": "An AI agent coding skeptic tries AI agent coding, in excessive detail",
    "url": "https://minimaxir.com/2026/02/ai-agent-coding/",
    "published_at": "2026-02-27T18:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "![](https://minimaxir.com/2026/02/ai-agent-coding/featured.png)\n\nYou’ve likely seen many blog posts about AI agent coding/vibecoding where the author talks about all the wonderful things agents can now do supported by vague anecdata, how agents will lead to the atrophy of programming skills, how agents impugn the sovereignty of the human soul, etc etc. This is **NOT** one of those posts. You’ve been warned.\n\nLast May, I wrote a blog post titled As an Experienced LLM User, I Actually Don’t Use Generative LLMs Often as a contrasting response to the hype around the rising popularity of agentic coding. In that post, I noted that while LLMs are most definitely not useless and they can answer simple coding questions faster than it would take for me to write it myself with sufficient accuracy, agents are a tougher sell: they are unpredictable, expensive, and the hype around it was wildly disproportionate given the results I had seen in personal usage. However, I concluded that I was open to agents if LLMs improved enough such that all my concerns were addressed and agents were more dependable.\n\nIn the months since, I continued my real-life work as a Data Scientist while keeping up-to-date on the latest LLMs popping up on OpenRouter. In August, Google announced the release of their Nano Banana generative image AI with a corresponding API that’s difficult to use, so I open-sourced the gemimg Python package that serves as an API wrapper. It’s not a thrilling project: there’s little room or need for creative implementation and my satisfaction with it was the net present value with what it enabled rather than writing the tool itself. Therefore as an experiment, I plopped the feature-complete code into various up-and-coming LLMs on OpenRouter and prompted the models to identify and fix any issues with the Python code: if it failed, it’s a good test for the current capabilities of LLMs, if it succeeded, then it’s a software quality increase for potential users of the package and I have no moral objection to it. The LLMs actually were helpful: in addition to adding good function docstrings and type hints, it identified more Pythonic implementations of various code blocks.\n\nAround this time, my coworkers were pushing GitHub Copilot within Visual Studio Code as a coding aid, particularly around then-new Claude Sonnet 4.5. For my data science work, Sonnet 4.5 in Copilot was not helpful and tended to create overly verbose Jupyter Notebooks so I was not impressed. However, in November, Google then released Nano Banana Pro which necessitated an immediate update to `gemimg` for compatibility with the model. After experimenting with Nano Banana Pro, I discovered that the model can create images with arbitrary grids (e.g. 2x2, 3x2) as an extremely practical workflow, so I quickly wrote a spec to implement support and also slice each subimage out of it to save individually. I knew this workflow is relatively simple-but-tedious to implement using Pillow shenanigans, so I felt safe enough to ask Copilot to `Create a grid.py file that implements the Grid class as described in issue #15`, and it did just that although with some errors in areas not mentioned in the spec (e.g. mixing row/column order) but they were easily fixed with more specific prompting. Even accounting for handling errors, that’s enough of a material productivity gain to be more *optimistic* of agent capabilities, but not nearly enough to become an AI hypester.\n\nIn November, just a few days before Thanksgiving, Anthropic released Claude Opus 4.5 and naturally my coworkers were curious if it was a significant improvement over Sonnet 4.5. It was very suspicious that Anthropic released Opus 4.5 right before a major holiday since companies typically do that in order to bury underwhelming announcements as your prospective users will be too busy gathering with family and friends to notice. Fortunately, I had no friends and no family in San Francisco so I had plenty of bandwidth to test the new Opus.\n\n## A Foreword on AGENTS.md#\n\nOne aspect of agents I hadn’t researched but knew was necessary to getting good results from agents was the concept of the AGENTS.md file: a file which can control specific behaviors of the agents such as code formatting. If the file is present in the project root, the agent will automatically read the file and in theory obey all the rules within. This is analogous to system prompts for normal LLM calls and if you’ve been following my writing, I have an unhealthy addiction to highly nuanced system prompts with additional shenanigans such as ALL CAPS for increased adherence to more important rules (yes, that’s still effective). I could not find a good starting point for a Python-oriented `AGENTS.md` I liked, so I asked Opus 4.5 to make one:\n\n```\nAdd an `AGENTS.md` file oriented for good Python code quality. It should be intricately details. More important rules should use caps, e.g. `MUST`\n```\n\nI then added a few more personal preferences and suggested tools from my previous failures working with agents in Python: use `uv` and `.venv` instead of the base Python installation, use `polars` instead of `pandas` for data manipulation, only store secrets/API keys/passwords in `.env` while ensuring `.env` is in `.gitignore`, etc. Most of these constraints don’t tell the agent what to do, but *how* to do it. In general, adding a rule to my `AGENTS.md` whenever I encounter a fundamental behavior I don’t like has been very effective. For example, agents love using unnecessary emoji which I hate, so I added a rule:\n\n```\n**NEVER** use emoji, or unicode that emulates emoji (e.g. ✓, ✗).\n```\n\nAgents also tend to leave a lot of redundant code comments, so I added another rule to prevent that:\n\n```\n**MUST** avoid including redundant comments which are tautological or self-demonstating (e.g. cases where it is easily parsable what the code does at a glance or its function name giving sufficient information as to what the code does, so the comment does nothing other than waste user time)\n```\n\nMy up-to-date `AGENTS.md` file for Python is available here, and throughout my time working with Opus, it adheres to every rule despite the file’s length, and in the instances where I accidentally query an agent without having an `AGENTS.md`, it’s *very* evident. It would not surprise me if the file is the main differentiator between those getting good and bad results with agents, although success is often mixed.\n\nAs a side note if you are using Claude Code, the file must be named `CLAUDE.md` instead because Anthropic is weird; this blog post will just use `AGENTS.md` for consistency.\n\n## Opus First Contact#\n\nWith my `AGENTS.md` file set up, I did more research into proper methods of prompting agents to see if I was missing something that led to the poor performance from working with Sonnet 4.5.\n\n![From the Claude Code quickstart.](claude_docs.png)\n\nFrom the Claude Code quickstart.\n\nAnthropic’s prompt suggestions are simple, but you can’t give an LLM an open-ended question like that and expect the results *you* want! You, the user, are likely subconsciously picky, and there are always functional requirements that the agent won’t magically apply because it cannot read minds and behaves as a literal genie. My approach to prompting is to write the potentially-very-large individual prompt in its own Markdown file (which can be tracked in `git`), then tag the agent with that prompt and tell it to implement that Markdown file. Once the work is completed and manually reviewed, I manually commit the work to `git`, with the message referencing the specific prompt file so I have good internal tracking.\n\n![](implement.png)\n\nI completely ignored Anthropic’s advice and wrote a more elaborate test prompt based on a use case I’m familiar with and therefore can audit the agent’s code quality. In 2021, I wrote a script to scrape YouTube video metadata from videos on a given channel using YouTube’s Data API, but the API is poorly and counterintuitively documented and my Python scripts aren’t great. I subscribe to the SiIvagunner YouTube account which, as a part of the channel’s gimmick (musical swaps with different melodies than the ones expected), posts hundreds of videos per month with nondescript thumbnails and titles, making it nonobvious which videos are the best other than the view counts. The video metadata could be used to surface good videos I missed, so I had a fun idea to test Opus 4.5:\n\n```\nCreate a robust Python script that, given a YouTube Channel ID, can scrape the YouTube Data API and store all video metadata in a SQLite database. The YOUTUBE_API_KEY is present in `.env`.\n\nDocumentation on the channel endpoint: https://developers.google.com/youtube/v3/guides/implementation/channels\n\nThe test channel ID to scrape is: `UC9ecwl3FTG66jIKA9JRDtmg`\n\nYou MUST obey ALL the FOLLOWING rules in your implementation.\n\n- Do not use the Google Client SDK. Use the REST API with `httpx`.\n- Include sensible aggregate metrics, e.g. number of comments on the video.\n- Incude `channel_id` and `retrieved_at` in the database schema.\n```\n\nThe resulting script is available here, and it worked first try to scrape up to 20,000 videos (the max limit). The resulting Python script has very Pythonic code quality following the copious rules provided by the `AGENTS.md`, and it’s more robust than my old script from 2021. It is most definitely not the type of output I encountered with Sonnet 4.5. There was a minor issue however: the logging is implemented naively such that the API key is leaked in the console. I added a rule to `AGENTS.md` but really this is the YouTube API’s fault for encouraging API keys as parameters in a GET request.\n\nI asked a more data-science-oriented followup prompt to test Opus 4.5’s skill at data-sciencing:\n\n```\nCreate a Jupyter Notebook that, using `polars` to process the data, does a thorough exploratory data analysis of data saved in `youtube_videos.db`, for all columns.\n\nThis analysis should be able to be extended to any arbitrary input `channel_id`.\n```\n\nThe resulting Jupyter Notebook is…indeed thorough. That’s on me for specifying “for all columns”, although it was able to infer the need for temporal analysis (e.g. total monthly video uploads over time) despite not explicitly being mentioned in the prompt.\n\nThe monthly analysis gave me an idea: could Opus 4.5 design a small webapp to view the top videos by month? That gives me the opportunity to try another test of how well Opus 4.5 works with less popular frameworks than React or other JavaScript component frameworks that LLMs push by default. Here, I’ll try FastAPI, Pico CSS for the front end (because we don’t need a JavaScript framework for this), and HTMX for lightweight client/server interactivity:\n\n```\nCreate a Hacker News-worthy FastAPI application using HTMX for interactivity and PicoCSS for styling to build a YouTube-themed application that leverages `youtube_videos.db` to create an interactive webpage that shows the top videos for each month, including embedded YouTube videos which can be clicked.\n```\n\n![](yt_web_app.webp)\n\nThe FastAPI webapp Python code is good with logical integration of HTMX routes and partials, but Opus 4.5 had fun with the “YouTube-themed” aspect of the prompt: the video thumbnail simulates a YouTube thumbnail with video duration that loads an embedded video player when clicked! The full code is open-source in this GitHub repository.\n\nAll of these tests performed far better than what I expected given my prior poor experiences with agents. Did I gaslight myself by being an agent skeptic? How did a LLM sent to die finally solve my agent problems? Despite the holiday, X and Hacker News were abuzz with similar stories about the massive difference between Sonnet 4.5 and Opus 4.5, so something *did* change.\n\nObviously an API scraper and data viewer alone do not justify an **OPUS 4.5 CHANGES EVERYTHING** declaration on social media, but it’s enough to be less cynical and more optimistic about agentic coding. It’s an invitation to continue creating more difficult tasks for Opus 4.5 to solve. From this point going forward, I will also switch to the terminal Claude Code, since my pipeline is simple enough and doesn’t warrant a UI or other shenanigans.\n\n## Getting Rusty At Coding#\n\nIf you’ve spent enough time on programming forums such as Hacker News, you’ve probably seen the name “Rust”, often in the context of snark. Rust is a relatively niche compiled programming language that touts two important features: speed, which is evident in framework benchmarks where it can perform 10x as fast as the fastest Python library, and memory safety enforced at compile time through its ownership and borrowing systems which mitigates many potential problems. For over a decade, the slogan “Rewrite it in Rust” became a meme where advocates argued that *everything* should be rewritten in Rust due to its benefits, including extremely mature software that’s infeasible to actually rewrite in a different language. Even the major LLM companies are looking to Rust to eke out as much performance as possible: OpenAI President Greg Brockman recently tweeted “rust is a perfect language for agents, given that if it compiles it’s ~correct” which — albeit that statement is silly at a technical level since code can still be *logically* incorrect — shows that OpenAI is very interested in Rust, and if they’re interested in writing Rust code, they need their LLMs to be able to code well in Rust.\n\nI myself am not very proficient in Rust. Rust has a famously excellent interactive tutorial, but a persistent issue with Rust is that there are few resources for those with intermediate knowledge: there’s little between the tutorial and “write an operating system from scratch.” That was around 2020 and I decided to wait and see if the ecosystem corrected this point (in 2026 it has not), but I’ve kept an eye on Hacker News for all the new Rust blog posts and library crates so that one day I too will be able to write the absolutely highest performing code possible.\n\nHistorically, LLMs have been poor at generating Rust code due to its nicheness relative to Python and JavaScript. Over the years, one of my test cases for evaluating new LLMs was to ask it to write a relatively simple application such as `Create a Rust app that can create \"word cloud\" data visualizations given a long input text.` but even without expert Rust knowledge I could tell the outputs were too simple and half-implemented to ever be functional even with additional prompting.\n\nHowever, due to modern LLM postraining paradigms, it’s entirely possible that newer LLMs are specifically RLHF-trained to write better code in Rust despite its relative scarcity. I ran more experiments with Opus 4.5 and using LLMs in Rust on some fun pet projects, and my results were *far* better than I expected. Here are four such projects:\n\n### icon-to-image#\n\nAs someone who primarily works in Python, what first caught ...",
    "summary": "No vagueposting here, just look at the Estimated Read Time.",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 12
    }
  },
  "74d5ebf4f1ad331a5fa1f35010f1dea248b0b814": {
    "id": "74d5ebf4f1ad331a5fa1f35010f1dea248b0b814",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "dfarq.homeip.net",
    "title": "What happened to GEM?",
    "url": "https://dfarq.homeip.net/whatever-happened-to-gem/?utm_source=rss&utm_medium=rss&utm_campaign=whatever-happened-to-gem",
    "published_at": "2026-02-27T12:00:07+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "Dave Farquhar Retro Computing   February 27, 2026February 27, 2026  2 Comments\n\nGEM was an early GUI for the IBM PC and compatibles and, later, the Atari ST, developed by Digital Research, the developers of CP/M and, later, DR-DOS. (Digital Equipment Corporation was a different company.) So what was it, and what happened to GEM?\n\nIt was very similar to the Apple Lisa, and Apple saw it as a Lisa/Macintosh ripoff and threatened to sue. While elements of GEM did indeed resemble the Lisa, Digital Research actually hired several developers from Xerox PARC.\n\nDRI demonstrated the 8086 version of GEM at COMDEX in 1984, and shipped it on 28 February 1985, beating Windows 1.0 to market by nearly 9 months.\n\n![What happened to GEM?](https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2012/08/what-happened-to-gem.jpg?resize=300%2C187&ssl=1)\n\nWhat happened to GEM? While it never gained really widespread use on PCs, it did find a home on the Atari ST. Atari also shipped it with its IBM-compatible PCs.\n\nI read about GEM in the early 1980s, but didn’t actually see it until 1993 when I was in college. When using a friend’s 286, I spied a copy of GEM installed on the hard drive, so I booted it up. Having used a number of 1980s GUIs previously, I had no trouble figuring out GEM. The problem was the lack of software.\n\nI’m sure performance was an issue on the pokey 4.77-MHz 8088 CPUs that were common in 1985. On the 286 I was using in 1993, which probably was 10 or 12 MHz, the speed was tolerable.\n\n## Competing with Windows\n\nThe lack of speed and lack of software pretty much doomed GEM on the PC. Apple pressured DRI to remove some of the user elements, making GEM less elegant to use. DRI settled out of court and complied, making the PC version unnecessarily clunky.\n\nWindows didn’t do much better; it was 1990 before Windows, finally in version 3, gained widespread adoption and use. But DRI discontinued GEM in 1988, two years earlier.\n\nI don’t think it was coincidence. By 1990, the 486 CPU was out. Few people could afford it, but it existed, and that pushed down the prices of 286 and 386 CPUs. Windows 3.0 was marginal at best on anything but the fastest 286s, but ran fine on the 386, and in 1990, the 386 was reasonably affordable.\n\nIn 1990 a perfect storm happened: PCs fast enough to run Windows existed, and Windows got to be good enough for people to want to use it.\n\nOne could argue DRI bowed out too soon. Then again, it’s questionable whether it would have won against Windows anyway. Microsoft was the larger company and had OEM agreements with all of the major PC makers. GEM only came with PCs from Amstrad and Atari, neither of whom were big PC sellers in the United States. They did better in Europe, and that’s why GEM did better in Europe than it did here.\n\n## Finding refuge in the Atari ST\n\nIn the meantime, GEM survived on the Atari ST. With an 8 MHz Motorola 68000 as the baseline, speed wasn’t a terrible concern. The 8 MHz 68000 was roughly equivalent to an 8 MHz 80386SX, had such a chip existed. In 1985, it was hot stuff. Hardware-wise, the ST matched up closely to its contemporary Macs and outperformed the PCs of its day, making GEM performance on the ST pretty much a non-issue. And since GEM was the default environment for the ST from the date of its release, available software was less of an issue. Third parties were going to develop for the ST, so they were going to use GEM.\n\nStrangely, Apple didn’t sue Atari like they did DRI, and GEM on the Atari remained very Mac-like. I don’t know why Apple didn’t see Atari as a threat. Given what Jack Tramiel had done to the Apple II while heading up Commodore, Apple shouldn’t have wanted him competing with the Mac. Really the only thing that saved Apple from a repeat performance was Tramiel’s lack of understanding that the ST and its operating system needed refinement every few years. By the late 1980s, the ST line looked more dated than it needed to.\n\nBut the bigger problem was software piracy. Piracy was common on the ST, and that made developers less enthusiastic to continue ST development, and instead, they ported their good ST software to other machines. The ST eventually died due to lack of software as the platform aged. By the early 1990s, developing for PCs running Windows was more profitable, and a sufficiently powerful PC running Windows could match or exceed the ST both in performance and price, something that wasn’t true in 1985.\n\n## What happened to GEM? Niche uses\n\nAnd it’s not entirely fair to declare 8086 GEM’s dying date as 1988. It lived on for several years as a graphical runtime library for DOS, most famously used by Ventura Publisher, one of the more popular desktop publishing packages for PCs.\n\nIn hindsight, it’s possible to see what went wrong. Had DRI been supplying the underlying operating system to PC makers (call it CP/M, call it DOS, whatever) and convinced one or more of the large US PC makers to bundle GEM with their PCs, and had DRI developed application software that used GEM, it’s easy to imagine an alternate history where GEM thrived the same way Windows did, and perhaps did it a bit sooner, especially if GEM had one or more killer apps and drove demand for ATs that could run it.\n\nLinux vendor Caldera ended up owning the old Digital Research intellectual property. Caldera released GEM as open source under the GNU GPL in April 1999, which resulted in the open source projects FreeGEM and OpenGEM. It has not been actively developed since 2008, serving as an example of how open source isn’t a silver bullet. As it stands, GEM is mostly another part of Gary Kildall’s mystique, sadly.\n\nIf you found this post informative or helpful, please share it!\n\n* share\n* share\n* save  1\n* share\n* share\n* share  5\n* pocket\n* share\n* email\n* RSS feed\n\n![](https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&ssl=1)![](data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22%3E%3C/svg%3E)\n\nDave Farquhar\n\nDavid Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.\n\n### Like this:\n\nLike Loading...\n\n### *Related stories by Dave Farquhar*",
    "summary": "GEM was an early GUI for the IBM PC and compatibles and, later, the Atari ST, developed by Digital Research, the developers of CP/M and, later, DR-DOS. (Digital Equipment Corporation was a different company.) So what was it, and what The post What happened to GEM? appeared first on The Silicon Underground .",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 6
    }
  },
  "13c98bff9f11da74a809cd480089b91d2a1a2e4f": {
    "id": "13c98bff9f11da74a809cd480089b91d2a1a2e4f",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "filfre.net",
    "title": "This Week on The Analog Antiquarian",
    "url": "https://www.filfre.net/2026/02/this-week-on-the-analog-antiquarian/",
    "published_at": "2026-02-27T16:22:23+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "### The Digital Antiquarian\n\nA history of computer entertainment and digital culture by Jimmy Maher\n\n* Home\n* About Me\n* Ebooks\n* Hall of Fame\n* Table of Contents\n\nRSS\n\n← Gabriel Knight 3: Blood of the Sacred, Blood of the Damned\n\n# This Week on The Analog Antiquarian\n\n27\nFeb\n\n> Chapter 14: The Dialogue\n\nComments Off on This Week on The Analog Antiquarian\n\nPosted by Jimmy Maher on February 27, 2026 in Uncategorized\n\n← Gabriel Knight 3: Blood of the Sacred, Blood of the Damned\n\nComments are closed.\n\n* #### Support this Blog\n\n  **If you value this blog, please think about supporting it by becoming a Patreon patron or via a one-time PayPal donation. Thanks!**\n\n  ```\n\n  ```\n\n  ![](/misc/Patreon.png)\n\n    \n\n  ![](/misc/donate.en.png)\n* #### Digital vs. Analog\n\n  Visit this site’s companion site, The Analog Antiquarian, for chronicles of worldly wonders.\n* #### Ebook Library\n\n  Here you’ll find collected in ebook format for offline reading most of the articles already published on this site.\n* #### Hall of Fame\n\n  A chronological list of the games and other interactive curiosities that I’ve found most fun and interesting over the years.\n* #### Table of Contents\n\n  Read this blog in chronological order, like a book.\n* #### Social Media\n\n  Receive announcements of new articles by following DigiAntiquarian on Bluesky or on Mastodon.\n* #### My Books\n\n  ![](https://analog-antiquarian.net/wp-content/uploads/2025/10/magellan_small.jpg)  \n  ![](https://analog-antiquarian.net/wp-content/uploads/2024/04/sistine_small.jpg)  \n  ![](https://analog-antiquarian.net/misc/China_small.jpg)\n* #### My Projects\n\n  + The King of Shreds and Patches\n  + Filfre: A Windows IF Interpreter\n  + Let’s Tell a Story Together: A History of Interactive Fiction\n  + His Majesty’s Ship Impetuous\n  + Older IF Writings and Reviews\n  + Miscellaneous Juvenilia\n  + About Me\n* #### Search\n\n  Search\n* #### Calendar\n\n  February 2026\n\n  | M | T | W | T | F | S | S |\n  | --- | --- | --- | --- | --- | --- | --- |\n  |  | | | | | | 1 |\n  | 2 | 3 | 4 | 5 | 6 | 7 | 8 |\n  | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n  | 16 | 17 | 18 | 19 | 20 | 21 | 22 |\n  | 23 | 24 | 25 | 26 | 27 | 28 |  |\n* #### Email\n\n  maher AT filfre DOT net\n\nProudly powered by WordPress\nTheme: Choco by .css{mayo}.\n\nEntries (RSS) and Comments (RSS)",
    "summary": "Chapter 14: The Dialogue",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 2
    }
  },
  "aaf3f879492886affe92a0dae0130115b8c1228e": {
    "id": "aaf3f879492886affe92a0dae0130115b8c1228e",
    "source_type": "rss",
    "info_layer": "content",
    "source_name": "anildash.com",
    "title": "A Cookie for Dario? — Anthropic and selling death",
    "url": "https://anildash.com/2026/02/27/a-cookie-for-dario/",
    "published_at": "2026-02-28T00:00:00+00:00",
    "fetched_at": "2026-02-28T04:54:14.921363+00:00",
    "language": "en",
    "content": "A Cookie for Dario? — Anthropic and selling death\n28 Feb 2026\n2026-02-28\n2026-02-28\n/images/pexels-lisa-from-pexels-230325.jpg\nai, policy, culture, tech\nA big tech headline this week is Anthropic (makers of Claude, widely regarded as one of the best LLM platforms) resisting Secretary of Defense Pete Hegseth’s...\n10\n\nA big tech headline this week is Anthropic (makers of Claude, widely regarded as one of the best LLM platforms) resisting Secretary of Defense Pete Hegseth’s calls to modify their platform in order to enable it to support his commission of war crimes. As has become clear this week, Anthropic CEO Dario Amodei has declined to do so. The administration couches the request as an attempt to use the technology for “lawful purposes”, but given that they’ve also described their recent crimes as legal, this is obviously not a description that can be trusted.\n\nMany people have, understandably, rushed to praise Dario and Anthropic’s leadership for this decision. I’m not so sure we should be handing out a cookie just because someone is saying they’re not going to let their tech be used to cause extrajudicial deaths.\n\nTo be clear: I am glad that Dario, and presumably the entire Anthropic board of directors, have made this choice. However, I don’t think we need to be overly effusive in our praise. The bar cannot be set so impossibly low that we celebrate merely refusing to directly, intentionally enable war crimes like the repeated bombing of unknown targets in international waters, in direct violation of both U.S. and international law. This is, in fact, basic common sense, and it’s shocking and inexcusable that any other technology platform *would* enable a sitting official of any government to knowingly commit such crimes.\n\nWe have to hold the line on normalizing this stuff, and remind people where reality still lives. This means we can recognize it as a positive move when companies do the reasonable thing, but also know that *this is what we should expect*. It’s also good to note that companies may have *many* reasons that they don’t want to sell to the Pentagon in addition to the obvious moral qualms about enabling an unqualified TV host who’s drunkenly stumbling his way through playacting as Secretary of Defense (which they insist on dressing up as the “Department of War” — another lie).\n\n## Selling to the Pentagon sucks\n\nBeing on *any* federal procurement schedule as a technology vendor is a tedious nightmare. There’s endless paperwork and process, all falling squarely into the types of procedures that a fast-moving technology startup is likely to be particularly bad at completing, with very few staff members having had prior familiarity handling such challenges. Right now, Anthropic handles most of the worst parts of these issues through partners like Amazon and Palantir. Addressing more of these unique and tedious needs for a demanding customer like the Pentagon themselves would almost certainly require blowing up the product roadmap or hiring focus within Anthropic for months or more, potentially delaying the release of cool and interesting features in service of boring (or just plain evil) capabilities that would be of little interest to 99.9% of normal users. Worse, if they have to *build* these features, it could exhaust or antagonize a significant percentage of the very expensive, very finicky employees of the company.\n\nThis is a key part of the calculus for Anthropic. A big part of their entire brand within the tech industry, and a huge part of why they’re appreciated by coders (in addition to the capabilities of their technology), is that they’re the “we don’t totally suck” LLM company. Think of them as “woke-light”. Within tech, as there have been massive waves of rolling layoffs over the last few years, people have felt terrified and unsettled about their future job prospects, even at the biggest tech companies. The only opportunities that feel relatively stable are on big AI teams, and most people of conscience don’t want to work for the ones that threaten kids’ lives or well-being. That leaves Anthropic alone amongst the big names, other than maybe Google. And Google has laid off people *at least 17 times* in the last three years alone.\n\nSo, if you’re Dario, and you want to keep your employees happy, and maintain your brand as the AI company that doesn’t suck, and you don’t want to blow up your roadmap, and you don’t want to have to hire a bunch of pricey procurement consultants, and you can stay focused on your core enterprise market, *and* you can take the right moral stand? It’s a pretty straightforward decision. It’s almost, I would suggest, an easy decision.\n\n## How did we get here?\n\nWe’ve only allowed ourselves to lower the bar this far because so many of the most powerful voices in Silicon Valley have so completely embraced the authoritarian administration currently in power in the United States. Facebook’s role in enabling the Rohingya genocide truly served as a tipping point in the contemporary normalization of major tech companies enabling crimes against humanity that would have been unthinkable just a few years prior; we can’t picture a world where MySpace helped accelerate the Darfur genocide, because the Silicon Valley tech companies we know about today didn’t yet aspire to that level of political and social control. But there are deeper precedents: IBM provided technology that helped enable the horrors of the holocaust in Germany in the 1940s, and that served as the template for their work implementing apartheid in South Africa in the 1970s. IBM actually *bid* for the contract to build these products for the South African government. And the systems IBM built were still in place when Elon Musk, Peter Thiel, David Sacks and a number of other Silicon Valley tycoons all lived there during their formative years. Later, as they became the vaunted “PayPal Mafia”, today’s generation of Silicon Valley product managers were taught to look up to them, so it’s no surprise that their acolytes have helped create companies that enable mass persecution and surveillance. But it’s also why one of the first big displays of worker power in tech was when many across the industry stood up against contracts with ICE. That moment was also one of the catalyzing events that drove the tech tycoons into their group chats where they collectively decided that they needed to bring their workers to heel.\n\nAnd they’ve escalated since then. Now, the richest man in the world, who is CEO of a few of the biggest tech companies, including one of the most influential social networks — and a major defense vendor to the United States government — has been openly inciting civil war *for years* on the basis of his racist conspiracy theories. The other tech tycoons, who look to him as a role model, think they’re being reasonable by comparison in the fact that they’re only enabling mass violence indirectly. That’s shifted the public conversation into such an extreme direction that we think it’s a *debate* as to whether or not companies should be party to crimes against humanity, or whether they should automate war crimes. No, they shouldn’t. This isn’t hard.\n\nWe don’t have to set the bar this low. We have to remind each other that this isn’t *normal* for the world, and doesn’t have to be normal for tech. We have to keep repeating the truth about where things stand, because too many people have taken this twisted narrative and accepted it as being real. The majority of tech’s biggest leaders are acting and speaking far beyond the boundaries of decency or basic humanity, and it’s time to stop coddling their behavior or acting as if it’s tolerable.\n In the meantime, yes, we can note when one has the temerity to finally, finally do the right thing. And then? Let’s get back to work.",
    "summary": "A big tech headline this week is Anthropic (makers of Claude, widely regarded as one of the best LLM platforms) resisting Secretary of Defense Pete Hegseth’s calls to modify their platform in order to enable it to support his commission of war crimes . As has become clear this week, Anthropic CEO Dario Amodei has declined to do so . The administration couches the request as an attempt to use the technology for “lawful purposes”, but given that they’ve also described their recent crimes as legal,",
    "tags": [],
    "importance_score": 0.4,
    "metadata": {
      "reading_time_minutes": 7
    }
  }
}